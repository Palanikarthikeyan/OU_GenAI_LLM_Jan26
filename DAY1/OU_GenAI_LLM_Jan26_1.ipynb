{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296bd40-21b3-4b33-b311-ca218d5e3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "www.python.org -- download -- python - General python - There is no ML module  \n",
    "Vs\n",
    "https://www.anaconda.com/download -->python = General python + ML modules + DE modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63d2a18-48b6-4017-b04e-e157251a2267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206a5d7c-ff10-4210-b04a-12ba7a7f9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(15)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe16ad0f-9078-40f2-b832-ec9bf086bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bcf2d-6cc8-4c5f-af27-17a2e3dd9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c14a784-b211-4209-bcdf-a6ab4880c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.9.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: llama-index, llama-index-core, unstructured\n"
     ]
    }
   ],
   "source": [
    "! pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8ae8db-7a05-4f4b-8890-676c309e8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i likes to read machine learning algorithm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371740bc-d5b0-4f0f-b79a-191967afeeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74250b03-e6bd-4b60-90ba-08c9c8c50f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookupError\n",
    ">>> import nltk\n",
    ">>> nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac85ac4-6bc9-483d-b5c6-4a0388f528b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'likes', 'to', 'read', 'machine', 'learning', 'algorithm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff66e8e-9c92-4bd0-84bd-0b140fd9908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a4f28b-0f40-4bd5-8ca8-31260a8b14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c67e932-3257-497f-8ce2-848f15772ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''hello welcome,to gen ai training\n",
    "do activity ! to become nlt learning\n",
    "all the gen ai llm models are built on\n",
    "top of nlp'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a46b89-e022-4bde-87a8-f92b2edf2596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'gen',\n",
       " 'ai',\n",
       " 'training',\n",
       " 'do',\n",
       " 'activity',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'nlt',\n",
       " 'learning',\n",
       " 'all',\n",
       " 'the',\n",
       " 'gen',\n",
       " 'ai',\n",
       " 'llm',\n",
       " 'models',\n",
       " 'are',\n",
       " 'built',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'nlp']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebbac0a1-8a55-4c79-bcd9-8f6eecc9b91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello welcome,to gen ai training\\ndo activity !',\n",
       " 'to become nlt learning\\nall the gen ai llm models are built on\\ntop of nlp']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c12aa700-6bdd-4cac-b54f-ae7f6abda5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'this',\n",
       " ',',\n",
       " 'can',\n",
       " 'i',\n",
       " '?',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'def',\n",
       " \"'\",\n",
       " 'g',\n",
       " 'hi',\n",
       " ':',\n",
       " 'j']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"i can't do this, can i? ab'c def'g hi:j\"\n",
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5772e99-785e-4385-ab8d-ed100321dfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'do',\n",
       " 'this',\n",
       " ',',\n",
       " 'can',\n",
       " 'i',\n",
       " '?',\n",
       " 'ab',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'def',\n",
       " \"'\",\n",
       " 'g',\n",
       " 'hi',\n",
       " ':',\n",
       " 'j']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68397aaa-ea42-4b48-833d-824f7433821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming - reduce root word --- ex: learning  --- learn \n",
    "#           ================= // stem \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "obj_stem = PorterStemmer()\n",
    "obj_stem.stem(\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82579cc-8ec8-4609-9c89-c6a2b2432587",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk/ <== package \n",
    "  -- tokenize.py <== module\n",
    "        |->def word_tokenize()\n",
    "        |->def wordpunct_tokenize()\n",
    "  -- stem.py <== module\n",
    "        |->class PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425e97b9-af52-4b01-95cc-8dd20ad29e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stem.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ed30d2-38d2-4e1c-979c-6df43161ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_stem.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f34b976-1157-4caa-88f2-77e31fc3beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "# RegexpStemmer('Regxpattern') ->obj\n",
    "\n",
    "reg_stem = RegexpStemmer(\"sh$|s$|e$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3dca4c-dba8-43d6-9175-00141162f2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00ec9713-e3c3-4f85-9a1d-2a9f24d2e5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ba'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem(\"bash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e838f4-51b0-4e80-b43a-520a89638ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratulation'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a914e0bc-fd4f-4eb0-a9df-b66b19c372bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization - lemma\n",
    "# reducing root word - based on pos = v,av,..\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatize_obj = WordNetLemmatizer()\n",
    "lemmatize_obj.lemmatize(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20a6e99-a0ba-4a7b-88b7-03eea84e77d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_obj.lemmatize(\"eating\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77549cb2-196f-4877-8e50-c8fd1e538ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_obj.lemmatize(\"history\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcea476-b5dd-4a79-986a-3c7e4d052ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization\n",
    "|\n",
    "stem/lemma \n",
    "|\n",
    "the hotel food is good  --> vector size is 5 \n",
    "|\n",
    "apply stop words \n",
    "|\n",
    "// hotel food good -->vector size is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853879c-1a05-4cee-bbda-87f470168876",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopword -- NLP preprocessing\n",
    "========\n",
    "    |->not carring significant meaning \n",
    "Reduce noise\n",
    "-------------//reduce vector size - improve the processing efficiency\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2381d7de-56f5-486b-92ca-1d25c905681c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "#stopwords.words('french')\n",
    "#stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f532d-b9a5-4cc7-8275-7b1ddb3ff2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization ->token - reduce this token ->stemm ->lemma ->stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87abffa-b875-46e9-8083-d3c5bd23594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Given_list = [\"words\",\"eats\",\"eatern\",\"writing\",\n",
    "              \"writes\",\"programming\",\"program\",\n",
    "              \"history\",\"finally\",\"finalized\"]\n",
    "\n",
    "# import nltk module\n",
    "#         |--> PorterStemmer - root words\n",
    "#         |--> WordNetLemmatizer - pos = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf918d05-ee7c-4856-b9d8-18f4c9156100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words    ---->word\n",
      "eats    ---->eat\n",
      "eatern    ---->eatern\n",
      "writing    ---->write\n",
      "writes    ---->write\n",
      "programming    ---->program\n",
      "program    ---->program\n",
      "history    ---->histori\n",
      "finally    ---->final\n",
      "finalized    ---->final\n"
     ]
    }
   ],
   "source": [
    "Given_list = [\"words\",\"eats\",\"eatern\",\"writing\",\n",
    "              \"writes\",\"programming\",\"program\",\n",
    "              \"history\",\"finally\",\"finalized\"]\n",
    "\n",
    "stemm_obj = PorterStemmer()\n",
    "\n",
    "for var in Given_list:\n",
    "    print(f'{var}    ---->{stemm_obj.stem(var)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7504a054-7073-4d97-8c6e-c2581d5de110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words ---->word\n",
      "eats ---->eat\n",
      "eatern ---->eatern\n",
      "writing ---->write\n",
      "writes ---->write\n",
      "programming ---->program\n",
      "program ---->program\n",
      "history ---->history\n",
      "finally ---->finally\n",
      "finalized ---->finalize\n"
     ]
    }
   ],
   "source": [
    "Given_list = [\"words\",\"eats\",\"eatern\",\"writing\",\n",
    "              \"writes\",\"programming\",\"program\",\n",
    "              \"history\",\"finally\",\"finalized\"]\n",
    "\n",
    "lemm_obj = WordNetLemmatizer()\n",
    "\n",
    "for var in Given_list:\n",
    "    print(f'{var} ---->{lemm_obj.lemmatize(var,pos='v')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0402853b-57f7-4898-b721-c4e4ad2b9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'this is an example displying of list of or collection of stop words filters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a497ffd4-92d8-4456-8c9f-743ed107dfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'displying',\n",
       " 'of',\n",
       " 'list',\n",
       " 'of',\n",
       " 'or',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filters']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b9dbc08-7d7d-4c67-8991-1d79e38b5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "displying\n",
      "list\n",
      "collection\n",
      "stop\n",
      "words\n",
      "filters\n"
     ]
    }
   ],
   "source": [
    "for var in word_tokenize(msg):\n",
    "    if var not in stopwords.words('english'):\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b121bb2a-c4dc-4c95-8b91-a915d779d7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "displying\n",
      "list\n",
      "collection\n",
      "STOP\n",
      "words\n",
      "filters\n"
     ]
    }
   ],
   "source": [
    "msg = 'This is an Example displying OF list of or collection of STOP words filters'\n",
    "for var in word_tokenize(msg):\n",
    "    if var.lower() not in stopwords.words('english'):\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fcb2331-5d2d-4c2e-92c7-95a32d50b2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='STOP'\n",
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49655d53-de91-419d-a2b6-c1ed97bbb817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STOP'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a02a86f-a271-48b7-9072-8680933debb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'STOP'.lower() in ['stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e448910-a190-4e9e-b5c2-d712f3da59e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([['food'],['good'],['bad']])\n",
    "\n",
    "encoder_obj = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder_obj.fit_transform(data)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45f09f8c-115c-4154-8fab-c2de01857dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [\"likes\",\"read\",\"nlp\", \"applications\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88dfada0-7774-4857-a403-7620a7da91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41344592-d35c-499e-81bb-bfeaa7735441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c6559-2f06-48d9-bba5-166595321c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def f1():\n",
    "...     return 10\n",
    "...\n",
    ">>> f1()\n",
    "10\n",
    ">>> def f2():\n",
    "...     yield 10\n",
    "...\n",
    ">>> f2()\n",
    "<generator object f2 at 0x0000020C4D9C9A80>\n",
    ">>>\n",
    ">>> next(f2())\n",
    "10\n",
    ">>> for var in f2():\n",
    "...     print(var)\n",
    "...\n",
    "10\n",
    ">>> list(f2())\n",
    "[10]\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0a06b2a-dd6f-4134-8515-29c7aa675083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000001E3F4F05A40>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "d = [\"likes\",\"read\",\"nlp\", \"applications\",\"oracle\"]\n",
    "ngrams(d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "952da5b2-8457-407c-8c74-0e30df6ab5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes',), ('read',), ('nlp',), ('applications',), ('oracle',)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ff6323b-ed6a-4bc4-94b0-93f22018ba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read'),\n",
       " ('read', 'nlp'),\n",
       " ('nlp', 'applications'),\n",
       " ('applications', 'oracle')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5bd5ceb-90a2-4e9f-9095-b4c731d08c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp'),\n",
       " ('read', 'nlp', 'applications'),\n",
       " ('nlp', 'applications', 'oracle')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abe1f298-a9a3-4c60-abf7-077af7c08681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp', 'applications'),\n",
       " ('read', 'nlp', 'applications', 'oracle')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1990bcb-9346-45e5-a2b9-9b5f1e8c5c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp', 'applications', 'oracle')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4784ddf8-3c1c-4bc1-a65e-86b87ead295d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(d,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b7841-be77-4550-994d-0b0eb458601e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
