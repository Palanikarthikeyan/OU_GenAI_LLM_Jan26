{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f4ec23-a6cd-44ed-a702-a4a13c48ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f9d3cb-b5cb-40e6-9cf1-86a0b2773ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Langchain is a framework\",\"FAISS is vector index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6a8eaf-952a-4084-baae-bced91790613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "vector_db = FAISS.from_texts(texts,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b47e932-e7be-42aa-a419-f871930a9335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a framework'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.similarity_search(\"what is langchain\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d8168-ab3b-4d84-ba71-35972a4d2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://huggingface.co\n",
    "\n",
    "Vision model - image processing\n",
    "Speech model\n",
    "embedding model - llm - all-MiniLm,e5,bge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc4425-7671-409a-a546-fc494b24c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linux/mac os --> vi .env {enter}\n",
    "\n",
    "windows ->Gitbash -> vi .env {enter}\n",
    "\n",
    "HF_TOKEN=\"\"\n",
    "       ----//there is no space in = LHS,RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b77b62c-b09a-4a2e-bfaf-8d3be99926f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fdff8e2-8bd7-4609-9858-8977197afeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f72bdf-ffbc-435e-8077-9ea75591902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'C:\\\\Users\\\\karth\\\\.env <-- file is exists'\n",
    "                    |\n",
    "                    |--> API token details are stored this file\n",
    "\n",
    "\n",
    "E:\\> <== \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() --->False  <======== .env file is not exists \n",
    "\n",
    "import os\n",
    "os.getcwd() --> E:\\\\ \n",
    "|\n",
    "How to initialize Token to os environ ?\n",
    "                           ==========\n",
    "\n",
    "os.environ - dict type\n",
    "\n",
    "os.environ['newKey']=<Value> # dict - adding new data \n",
    "            |            |\n",
    "         HF_TOKEN        |<--- os.getenv('HF_TOKEN')\n",
    "                               ======================= \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0a421e-7a47-49df-b7e3-87d1b97c56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fa7a14-0321-4607-99db-910576a38685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0660fd1d-ea96-43ab-9da9-ace9411039e1",
   "metadata": {},
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embed_obj = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e4b35-0d45-4bee-98ee-4c3b9cd75464",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_obj.embed_query(\"hello\") -> list of vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f0d07c-3b95-4bf7-aa26-1d654ab204f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "# Step - 1\n",
    "from langchain_community.document_loaders import TextLoader \n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Step - 2\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "splitted_obj = CharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "splitted_docs = splitted_obj.split_documents(docs)\n",
    "\n",
    "# Step -3 \n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#embedded_obj = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedded_obj = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a35fc2-df2d-438d-a3c3-fe3421dc599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step-4\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vecro_db = FAISS.from_documents(splitted_docs,embedded_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b3df0e-f06f-4691-9877-8161776b3fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f0b202ba-ad8e-45e0-99ea-07c91aa48b14', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).'),\n",
       " Document(id='e1077c7c-e8fd-477d-a8ec-3bd12697bcaf', metadata={'source': 'my_docs.txt'}, page_content='LangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(id='895d1ff0-0ad0-4403-9fae-8b5996b0f019', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"),\n",
       " Document(id='35872bb1-6805-44fc-aa8d-231df74866a7', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecro_db.similarity_search('what is langchain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf58df-f0c2-4727-b717-4d6da2de3340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295cf6b-32f7-4b9b-b31b-d25a019dc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq \n",
    "  |--> LPU \n",
    "\n",
    "! pip install groq\n",
    "\n",
    "GROQ_API_KEY\n",
    "\n",
    "https://console.groq.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550ca5f1-9556-49bc-abc0-de04b9036bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: groq\n",
      "Version: 0.30.0\n",
      "Summary: The official Python library for the groq API\n",
      "Home-page: https://github.com/groq/groq-python\n",
      "Author: \n",
      "Author-email: Groq <support@groq.com>\n",
      "License: Apache-2.0\n",
      "Location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, typing-extensions\n",
      "Required-by: langchain-groq\n"
     ]
    }
   ],
   "source": [
    "! pip show groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b340b8cc-9abf-4386-93b5-8f62bf1b49eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06346fd1-6f8b-4a4a-8717-49498398e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc6c8fe-ea38-441c-a7ee-38cbda860ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "527245af-8337-467d-9a05-bc1a877b2159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangChain is an open-source, Python-based framework for building and composing large language models, also known as LLMs. It's designed to make it easier for developers to create, connect, and customize LLMs to solve complex tasks and problems. LangChain allows developers to use multiple LLMs in a modular way, which enables the creation of more advanced, hybrid models that can leverage the strengths of each individual model.\\n\\nSome of the key features and capabilities of LangChain include:\\n\\n1. **Modular architecture**: LangChain allows developers to break down complex tasks into smaller, more manageable components, each of which can be handled by a different LLM or a combination of LLMs.\\n2. **LLM composition**: LangChain provides a way to compose multiple LLMs together, enabling developers to create more powerful and flexible models that can handle a wide range of tasks.\\n3. **Data integration**: LangChain allows developers to easily integrate data from various sources, including databases, APIs, and file systems, into their LLM-based models.\\n4. **Model fine-tuning**: LangChain provides tools for fine-tuning pre-trained LLMs on custom datasets, enabling developers to adapt these models to their specific use cases.\\n5. **Model evaluation**: LangChain includes tools for evaluating the performance of LLM-based models, making it easier to compare different models and identify areas for improvement.\\n\\nSome of the potential applications of LangChain include:\\n\\n1. **Chatbots and conversational AI**: LangChain can be used to build more advanced chatbots that can understand and respond to complex user queries.\\n2. **Text summarization**: LangChain can be used to develop more accurate and informative text summarization systems.\\n3. **Question answering**: LangChain can be used to build more effective question answering systems that can handle a wide range of questions and topics.\\n4. **Sentiment analysis**: LangChain can be used to develop more accurate sentiment analysis systems that can analyze text and identify emotions.\\n\\nOverall, LangChain is a powerful tool for developers who want to create more advanced and flexible LLM-based models that can handle complex tasks and problems.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 433, 'prompt_tokens': 40, 'total_tokens': 473, 'completion_time': 0.627518434, 'prompt_time': 0.002191627, 'queue_time': 0.050509743, 'total_time': 0.629710061}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--71e74044-b5db-4eb9-ba49-123959e2bb1d-0' usage_metadata={'input_tokens': 40, 'output_tokens': 433, 'total_tokens': 473}\n"
     ]
    }
   ],
   "source": [
    "r = llm_obj.invoke(\"what is langchain?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5106e62c-8022-46a8-9568-d2a7b6b261c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Delhi is the capital city of India. It is located in the northern part of the country, in the National Capital Territory of Delhi (NCT Delhi).' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 39, 'total_tokens': 72, 'completion_time': 0.041785401, 'prompt_time': 0.001799204, 'queue_time': 0.050659456, 'total_time': 0.043584605}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--8c47a3a6-1992-4c6a-9690-88ee24b75d40-0' usage_metadata={'input_tokens': 39, 'output_tokens': 33, 'total_tokens': 72}\n"
     ]
    }
   ],
   "source": [
    "r = llm_obj.invoke(\"Where is Delhi?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1a8ff-0710-42b4-9ea4-5ff7d27b0165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b60a0-44d4-4482-8cfd-1a730152ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load the data\n",
    "|\n",
    "Chunk\n",
    "|\n",
    "Embedding_obj\n",
    "|\n",
    "VectorStore\n",
    "|\n",
    "VectorStore.similarity_search('query') ->..\n",
    "#################\n",
    "|\n",
    "Retriver\n",
    "|\n",
    "llm\n",
    "|\n",
    "Retriver_Chain\n",
    "|\n",
    "Prompt\n",
    "|\n",
    "Prompt|Retriver_Chain ->invoke('Query')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd11a6-a829-4328-8b94-af34553d0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### End of Day2 Session ################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
