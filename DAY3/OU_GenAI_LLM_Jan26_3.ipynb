{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96a1a3f-424f-4ee2-ab74-6ec6f4984629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dd44c8-bf47-40a4-ae91-1e5116dd2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.27\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "! pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cd542-3f81-457c-b6bb-82e9de204ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoading -->Chunks -->Embedding -->StorestoVectorDB  <--------- Retrieval\n",
    "                                            |->similarity_search\n",
    "\n",
    "\n",
    "RetrievalQA\n",
    " |-->Chain - pipeline ( process1 | process2 ) \n",
    "                            |_________|    \n",
    "\n",
    "Retrival_object \n",
    "llm_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c2aac1-914d-4b7a-8969-82a35f56f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA # Higher version of langchain -> from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f91824-528d-434f-ba0f-649dd4e1a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# step-1\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# step-2\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# step-3\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d7bf4e-eaa3-4814-93f5-858c02b6871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step -4\n",
    "vectorstore = FAISS.from_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3748c92d-e407-4ff9-b6e7-537b32872203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18eea66-1886-4a00-ab8f-8998e8d7dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-5\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriever object\n",
    "\n",
    "# Step-6 - create llm object\n",
    "llm_obj = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a6ad76-cdbf-4184-a146-0eb248b52bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-7 - Build Retrieval QA Chain \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef67ff-8d77-4d90-ba07-1eae886edab8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             print(\"OK\")\n",
    "...\n",
    ">>> obj = box()\n",
    "OK\n",
    ">>> callable(obj)\n",
    "False\n",
    ">>>\n",
    ">>> obj()\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "TypeError: 'box' object is not callable\n",
    ">>>\n",
    ">>> def fx():\n",
    "...     print(\"OK\")\n",
    "...\n",
    ">>> type(fx)\n",
    "<class 'function'>\n",
    ">>>\n",
    ">>> callable(fx)\n",
    "True\n",
    ">>> fx()\n",
    "OK\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             pass\n",
    "...     def __call__(self):\n",
    "...             print(\"OK\")\n",
    "...\n",
    ">>> obj = box()\n",
    ">>> callable(obj)\n",
    "True\n",
    ">>>\n",
    ">>> fx.__call__()  # same fx()\n",
    "OK\n",
    ">>>\n",
    ">>> obj()\n",
    "OK\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934d79a6-5f77-4a37-bf7b-ba6bf53fe699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_29792\\1664598314.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa_chain(\"what is langchain?\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the entire process of building, deploying, and managing LLM-based applications, making it easier to create and optimize these applications.\\n\\nLangChain provides an open-source platform that includes various components and integrations to help developers build stateful agents, monitor and evaluate their applications, and deploy them into production.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-8 QA_Chain ->invoke a query\n",
    "#qa_chain <-- callable object\n",
    "qa_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aeb0be6-efdf-4fc4-bc75-9de83cef0595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write hello world example program in C ?',\n",
       " 'result': 'I don\\'t know how to write a \"Hello World\" example program in C using the context of LangChain, as LangChain seems to be related to large language models and application development, not a programming language like C. However, I can provide a simple \"Hello World\" example program in C:\\n\\n```c\\n#include <stdio.h>\\n\\nint main() {\\n    printf(\"Hello, World!\\\\n\");\\n    return 0;\\n}\\n```\\n\\nTo compile and run this program, you would typically use a C compiler like GCC (GNU Compiler Collection) from the command line:\\n\\n```bash\\ngcc hello.c -o hello\\n./hello\\n```\\n\\nThis will compile the `hello.c` file into an executable named `hello`, and then run the executable, printing \"Hello, World!\" to the console.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write hello world example program in C ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47995cf-215b-4a18-a097-ebdc91ab6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\langchain\\\\__init__.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838a2d65-3df2-4fc3-bc68-a699ede17b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c7edfa-d02d-4f96-9cc3-c7361b6865ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\os.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bc0f61-206f-4957-8e6c-d15baf008a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\requests\\\\__init__.py'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e01098-dfa5-4009-8d83-659bc2fae618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca35d97-c8e8-414e-b3c1-452a5d0b53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompting\n",
    "---------\n",
    " |->prompt  - text \n",
    " |->jinja2 - template code ---> {{variable}} \n",
    "\n",
    " |--> instructing an LLM to give the result/output user wants. //How to talk to AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15495d4f-115c-4808-9f9c-6367108ad4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4601f733-e5bb-401c-8c11-56efe1408855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read java book')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read java book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab826a3-f9fe-49d2-8dc0-9ce0a86ddff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create dynamic value -> template placeholder/variable {variable/placeholder} \n",
    "# PromptTemplate.from_template('user defined prompt') ->object\n",
    "# object.format(userdefined_input_placeholder=Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd6fb4d0-5eb8-4667-b9d9-74da71703ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read html book'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "obj.format(myvar=\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb72f54-8a34-457c-999c-c48810fc1990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read story book'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b48347-5ebc-409b-8280-b1c130f9b939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5236a721-96ed-4ea8-89cd-5cbb3109906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['author', 'my_year', 'myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book written by {author} released on {my_year}')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book written by {author} released on {my_year}')\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70e9ed35-8abe-4257-b59d-917bcc7378b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read birds book written by Mr.ABC released on 2004'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"birds\",author=\"Mr.ABC\",my_year=2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5f4e0-51be-4929-a1a1-f52215d66211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate\n",
    "------------------\n",
    "|->Langchain utility \n",
    "|->Build structured prompts\n",
    "        ===================\n",
    "               |->Chat bot , QA \n",
    "\n",
    "\n",
    "End_User: Query ---------->--- | Application |  <== Human message - userQuery (or) userinput\n",
    "                                 ..\n",
    "            =======<==========                  <== AI message \n",
    "                Response                                   |<== System message - rules/behavior  \n",
    "\n",
    "\n",
    "1. Role/Context\n",
    "2. Task \n",
    "3. Format - how should the output/result look ?\n",
    "     (ex: provide the answer as a bullet point\n",
    "           write it in json format \n",
    "           keep it under 100 words..)\n",
    "\n",
    "4. Constraints - What rules to follow?\n",
    "      (ex: Don't use technical jargon\n",
    "           use python 3.10+ features)\n",
    "\n",
    "5. Example - optional \n",
    "           \n",
    "List of tuple \n",
    "--------------\n",
    "[(\"system\",define role),(\"human\",userQuery)] \n",
    "\n",
    "Example:\n",
    "[(\"system\",\"You are a helpful AI assistant.\"),\n",
    " (\"human\",\"Explain about {topic} in simple terms\")]\n",
    "|\n",
    "|\n",
    "Langchain Converts into \n",
    "System: \n",
    "Human: \n",
    "\n",
    "UserInput is:  topic=\"transformers\"\n",
    "|\n",
    "|\n",
    "Langchain Converts to\n",
    "|\n",
    "System: You are a helpful AI assistant.\n",
    "Human:  Explain about transformers in simple terms\n",
    "---------------------------------------------------\n",
    " |\n",
    " |_______ sends to chat model\n",
    "\n",
    "system , You are a helpful AI bot. -> SystemMessage(content=\"You are a helpful AI bot.\"....)\n",
    "human, Hello, how are you doing? -> HumanMessage(content=\"Hello, how are you doing?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0c910d-422e-4d5b-9290-333d37c998c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da427f1d-9121-4e77-b2ed-22ec00cceae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc97d6ee-d38a-4927-a5ef-64acef6620a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python books'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read python books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7de84653-75d6-428a-abbc-36c04d54a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['mybook'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['mybook'], input_types={}, partial_variables={}, template='I likes to read {mybook} books'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "844ecd52-2a62-4d86-a4a1-9803dcc2c4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read ruby books', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books')\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(mybook=\"ruby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814a2cc-3cf0-4fc8-87d9-903d7520d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.invoke(\"what is langchain?\") --->HumanMessage(content=\"what is langchain?\",.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1792a0c4-f9e4-4d10-bb51-025e0177b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read ruby books', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books'),\n",
    "        ('ai','Sure')\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(mybook=\"ruby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5092d5a2-e08e-4d5b-a1f6-cfa4bb9c0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_29792\\275367955.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  my_llm_obj = Ollama(model=\"gemma2:2b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7153b19c-ed2f-4bd9-a073-a8392efdee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84809a11-cb0b-42b3-8a87-873504aa8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "197c4e60-fbdd-409b-b51b-9b12225d279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GenAI, or **Generative Artificial Intelligence**, is a type of artificial intelligence that focuses on creating new content.  \\n\\nHere's a breakdown:\\n\\n**What it does:**\\n\\n* **Creates original work**: This can range from text and images to music, code, videos, and even 3D models.\\n* **Learns patterns and relationships**: GenAI uses vast datasets of information to understand how things are connected and generate new content based on learned patterns.  \\n* **Applies creative techniques**: It can emulate human creativity by producing diverse, original output in various styles and formats.\\n\\n**How it works:**\\n\\nGenAI usually relies on powerful algorithms that process massive amounts of data to train models like:\\n\\n* **Language Models**: GPT-3, Bard, etc. excel at understanding and generating human-like text.\\n* **Image Models**: DALL-E 2, Midjourney, etc. can create realistic or imaginative images from text prompts. \\n\\n**Examples:**\\n\\n* **Writing Assistant**:  Tools like Jasper or Copy.ai help users generate content quickly and efficiently for writing blog posts, social media updates, etc.\\n* **Image Generation**: Artists are using AI tools to design unique artwork, while marketing teams use them to create custom visuals for campaigns.\\n* **Code Generation**: Tools like GitHub Copilot can suggest code snippets based on user prompts, accelerating the development process. \\n\\n**Overall:** GenAI is revolutionizing content creation and opening up new possibilities in various industries!\\n\\n\\nLet me know if you have any more questions about GenAI!  ðŸ˜Š \\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e404909-09e7-48f4-9c78-960deedc5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Say don't know\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acec117a-5fb7-4f8f-a7f2-e1b5b3747d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1105b21a-be81-40fd-8b19-9cada9560e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Say: Don't know \\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5da6c63-1548-4feb-a922-8e51aae3551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])\n",
    "my_chain = chat_prompt|llm_obj # groq object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb71006e-23d5-4725-a98a-431c773d3da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='GenAI, short for Generalized Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, reason, and apply its knowledge across a wide range of tasks, similar to human intelligence.\\n\\nGenAI is often considered a type of artificial general intelligence (AGI), which is a more specific term that refers to an AI system that has the ability to understand, learn, and apply its knowledge to solve any problem, in the same way that a human would.\\n\\nThe key characteristics of GenAI include:\\n\\n1. **General knowledge**: Ability to understand and apply knowledge across various domains and tasks.\\n2. **Reasoning and problem-solving**: Ability to reason, solve problems, and make decisions, like a human would.\\n3. **Learning and adaptation**: Ability to learn from experience, adapt to new situations, and improve over time.\\n4. **Self-awareness and consciousness**: Some researchers believe that GenAI may also possess self-awareness and consciousness, although this is still a topic of debate.\\n\\nGenAI is considered a significant milestone in the development of artificial intelligence, as it would enable AI systems to:\\n\\n1. **Assist humans**: GenAI could assist humans in various tasks, such as healthcare, finance, education, and more.\\n2. **Create new technologies**: GenAI could generate new ideas, products, and services that would transform industries and society.\\n3. **Improve decision-making**: GenAI could provide humans with more accurate and informed decision-making capabilities.\\n\\nHowever, the development of GenAI also raises significant concerns, such as:\\n\\n1. **Job displacement**: GenAI could automate many jobs, potentially leading to significant job displacement.\\n2. **Bias and ethics**: GenAI could perpetuate biases and make decisions that are unfair or unethical.\\n3. **Security risks**: GenAI could pose significant security risks if not designed and developed with robust safety protocols.\\n\\nWhile researchers have made significant progress in developing narrow AI systems, the development of GenAI remains a subject of ongoing research and debate.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 45, 'total_tokens': 452, 'completion_time': 0.536295079, 'prompt_time': 0.00215373, 'queue_time': 0.050621596, 'total_time': 0.538448809}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--87056912-0d30-42b6-a48b-72dffe6d3059-0', usage_metadata={'input_tokens': 45, 'output_tokens': 407, 'total_tokens': 452})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3530b3ce-db2b-4e92-9efa-b78b9971d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your sql expert\"),\n",
    "    (\"user\",\"Get list of users from {question} dept\"),\n",
    "    (\"ai\",\"select *from users\")\n",
    "])\n",
    "my_chain = chat_prompt|llm_obj # groq object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "418c8cdb-15da-41e6-841b-1479e6c8444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" \\nwhere department = 'Sales'\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56, 'completion_time': 0.02777622, 'prompt_time': 0.002276861, 'queue_time': 0.050675056, 'total_time': 0.030053081}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7b291980-26c0-4d24-8e5d-af039b4ea891-0', usage_metadata={'input_tokens': 48, 'output_tokens': 8, 'total_tokens': 56})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'sales'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897f592-b42b-4b09-bc59-0b3dc406d665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45811b53-03df-4057-972d-18b86b10844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization\n",
    "text = '''\n",
    "Aritfical Intelligence(AI) is transforming industries..\n",
    "AI models are used in hearthcare,finance,education,enterprise indus and also supports autonomous system'''\n",
    "\n",
    "prompt = f'Summarize the following text in 3 bullet points:\\n{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "119f2cb9-93ca-4985-8acf-e11264f5e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 3-bullet point summary of the text:\n",
      "\n",
      "* **Artificial Intelligence (AI) is impacting diverse industries.** AI models are being utilized in fields such as healthcare, finance, education, enterprise operations, and even support for autonomous systems.\n",
      "* **AI's influence extends across various sectors.**  The text highlights the widespread application of AI across different industries, showcasing its versatility and reach. \n",
      "* **Key applications include healthcare, finance, and education.** The text specifically mentions how AI is used in these areas as examples of the impact of AI on daily life. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_llm_obj.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa2682-5818-4e60-815e-f59ca5d2081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''Summarize the following text in 3 bullet points:\\n{text}\n",
    "           1. data1\n",
    "           2. data2\n",
    "           3. data3'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b996e-e2e4-4f07-8f2d-0315ecbfcad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334a28c-1366-4838-b719-7ea0b2227fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "# Grammar and Text Correction\n",
    "# Text to structured \n",
    "# Code generation\n",
    "# Content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bab4d6d-be4b-47f9-8761-426530d14be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s the corrected text:\\n\\n\"AI is changing the world in many ways. It helps people to work faster.\"\\n\\nHere\\'s a breakdown of the corrections:\\n\\n- \"AI are\" should be \"AI is\" (subject-verb agreement: AI is a singular subject, so it should use a singular verb)\\n- \"chaining\" should be \"changing\" (correct spelling)\\n- \"the world in many way\" should be \"the world in many ways\" (plural form of \"way\")\\n- \"It help\" should be \"It helps\" (subject-verb agreement: a singular subject \"it\" should use a singular verb \"helps\")\\n- \"peoples\" should be \"people\" (correct spelling)' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 60, 'total_tokens': 207, 'completion_time': 0.209621642, 'prompt_time': 0.00295258, 'queue_time': 0.06125126, 'total_time': 0.212574222}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--3057fa92-f402-4bca-8451-a2e7c50791dc-0' usage_metadata={'input_tokens': 60, 'output_tokens': 147, 'total_tokens': 207}\n"
     ]
    }
   ],
   "source": [
    "# Grammar and Text Correction\n",
    "prompt = '''\n",
    "Correct grammar and spelling mistakes in this text:\n",
    "\"AI are chaining the world in many way.It help peoples to work faster\"\n",
    "'''\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25c3ba4e-7f92-445d-bffc-c57ea2f6c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the corrected text:\n",
      "\n",
      "\"AI is changing the world in many ways. It helps people to work faster.\"\n",
      "\n",
      "Here's a breakdown of the corrections:\n",
      "\n",
      "- \"AI are\" should be \"AI is\" (subject-verb agreement: AI is a singular subject, so it should use a singular verb)\n",
      "- \"chaining\" should be \"changing\" (correct spelling)\n",
      "- \"the world in many way\" should be \"the world in many ways\" (plural form of \"way\")\n",
      "- \"It help\" should be \"It helps\" (subject-verb agreement: a singular subject \"it\" should use a singular verb \"helps\")\n",
      "- \"peoples\" should be \"people\" (correct spelling)\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "262056ff-8e42-4921-9d6b-b2f14606a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 bullet points summarizing the text:\n",
      "\n",
      "â€¢ LangChain is an open-source framework for developing applications powered by large language models (LLMs).\n",
      "â€¢ The framework simplifies the entire lifecycle of LLM application development, including development, productionization, and deployment.\n",
      "â€¢ During development, LangChain provides open-source components and third-party integrations to build and customize applications.\n",
      "â€¢ LangSmith is used for productionization, allowing developers to inspect, monitor, and evaluate applications for continuous optimization and deployment.\n",
      "â€¢ LangChain's LangGraph Platform enables the deployment of LangGraph applications as production-ready APIs and Assistants.\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "# Task \n",
    "# read my_docs.txt file - use python file handling - open a inputfile - read a content - pass this input to prompt\n",
    "# define prompt - Summarize 5 bullet points \n",
    "\n",
    "s  = open('my_docs.txt').read()\n",
    "\n",
    "prompt = f'Summarize the following text in 5 bullet points:\\n\\n{s}'\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbf7d703-9701-4e6e-80f5-147ae0bf2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see any given text. Please provide the text, and I will extract the information and present it in JSON format.\n",
      "\n",
      "However, if you provide the text \"Ctpl indus was founded by Mr.Ram in year 1982 in Bangalore\", here's the extracted information in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"Company Name\": \"Ctpl indus\",\n",
      "    \"Founder\": \"Mr. Ram\",\n",
      "    \"Year Founded\": 1982,\n",
      "    \"Location\": \"Bangalore\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Text to structured \n",
    "text = \"Ctpl indus was founded by Mr.Ram in year 1982 in Bangalore\"\n",
    "\n",
    "prompt = f'From the given text:{text} extract information in json'\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0a09686-a0d8-44cd-b361-40fb3be618f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Factorial of a Number in Python**\n",
      "====================================================\n",
      "\n",
      "Here's a simple Python function that calculates the factorial of a given number:\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number for which the factorial is to be calculated.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input number is negative.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input number must be non-negative.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * calculate_factorial(n-1)\n",
      "```\n",
      "**Example Use Cases**\n",
      "--------------------\n",
      "\n",
      "```python\n",
      "# Calculate the factorial of 5\n",
      "print(calculate_factorial(5))  # Output: 120\n",
      "\n",
      "# Calculate the factorial of 0\n",
      "print(calculate_factorial(0))  # Output: 1\n",
      "\n",
      "# Calculate the factorial of a negative number (raises ValueError)\n",
      "try:\n",
      "    print(calculate_factorial(-5))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Input number must be non-negative.\n",
      "\n",
      "# Calculate the factorial of a non-integer (raises TypeError)\n",
      "try:\n",
      "    print(calculate_factorial(3.5))\n",
      "except TypeError as e:\n",
      "    print(e)  # Output: Input must be an integer.\n",
      "```\n",
      "**Alternative Recursive Implementation**\n",
      "--------------------------------------\n",
      "\n",
      "For a more efficient recursive implementation, you can use memoization to store the factorial of previously calculated numbers:\n",
      "```python\n",
      "factorial_cache = {0: 1, 1: 1}\n",
      "\n",
      "def calculate_factorial(n):\n",
      "    if n not in factorial_cache:\n",
      "        factorial_cache[n] = n * calculate_factorial(n-1)\n",
      "    return factorial_cache[n]\n",
      "```\n",
      "This approach avoids redundant calculations and improves performance for large inputs. However, it uses more memory to store the cache.\n",
      "\n",
      "**Iterative Implementation**\n",
      "---------------------------\n",
      "\n",
      "For an iterative approach, you can use a simple loop to calculate the factorial:\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input number must be non-negative.\")\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "This implementation is more efficient than the recursive approach and does not use any extra memory.\n"
     ]
    }
   ],
   "source": [
    "# # Code generation\n",
    "\n",
    "input_ = 'Write a python function to calculate factorial of a number'\n",
    "response = llm_obj.invoke(input_)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1ca79a7-782d-496b-b8d0-f949de928f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the text:\n",
      "\n",
      "\"Our product is affordable and useful for every customer, whether in a formal or professional setting.\"\n"
     ]
    }
   ],
   "source": [
    "# Text -->Rewrite \n",
    "text = \"Our product is afforable and useful for every customers\"\n",
    "\n",
    "prompt = f\"Rewrite the give text:{text} in formal and profession:\"\n",
    "\n",
    "print(llm_obj.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3113a6df-f3a3-404e-930e-a0a140147881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample email for a professional appointment notification:\n",
      "\n",
      "Subject: Job Offer and Appointment Notification\n",
      "\n",
      "Dear [Candidate Name],\n",
      "\n",
      "We are pleased to inform you that after a thorough evaluation of your application and interview process, we are delighted to offer you the position of [Job Title] at [Company Name]. Your exceptional skills, experience, and passion for the field made you stand out among other candidates, and we believe you would be a valuable addition to our team.\n",
      "\n",
      "Below are the details of your appointment:\n",
      "\n",
      "- Job Title: [Job Title]\n",
      "- Department: [Department Name]\n",
      "- Reporting Manager: [Manager's Name]\n",
      "- Start Date: [Start Date]\n",
      "- Salary: [Salary Amount]\n",
      "- Benefits: [List of benefits, if applicable]\n",
      "\n",
      "We have attached a copy of the employment contract, which outlines the terms and conditions of your appointment. Please review it carefully and do not hesitate to reach out to us if you have any questions or concerns.\n",
      "\n",
      "To confirm your acceptance, please sign and return a copy of the contract to us by [Deadline for response]. We also require a start date confirmation by [Deadline for response].\n",
      "\n",
      "We look forward to welcoming you to our team and working together to achieve great things. If you have any questions or need further information, please do not hesitate to contact us.\n",
      "\n",
      "Congratulations once again on your appointment, and we look forward to hearing back from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Title]\n",
      "[Company Name]\n",
      "[Contact Information]\n",
      "\n",
      "Attachments: \n",
      "- Employment Contract\n",
      "- Job Description\n",
      "- Company Policies and Procedures (if applicable)\n",
      "\n",
      "Please note that you should customize this email to fit your company's specific needs and requirements.\n"
     ]
    }
   ],
   "source": [
    "input_var = 'Give me professional appointment email for a candidate who was selected'\n",
    "print(llm_obj.invoke(input_var).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2b36a-9528-4756-8233-198c1dd0dbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
