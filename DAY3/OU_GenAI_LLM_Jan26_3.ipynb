{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96a1a3f-424f-4ee2-ab74-6ec6f4984629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dd44c8-bf47-40a4-ae91-1e5116dd2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.27\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "! pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cd542-3f81-457c-b6bb-82e9de204ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoading -->Chunks -->Embedding -->StorestoVectorDB  <--------- Retrieval\n",
    "                                            |->similarity_search\n",
    "\n",
    "\n",
    "RetrievalQA\n",
    " |-->Chain - pipeline ( process1 | process2 ) \n",
    "                            |_________|    \n",
    "\n",
    "Retrival_object \n",
    "llm_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c2aac1-914d-4b7a-8969-82a35f56f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA # Higher version of langchain -> from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f91824-528d-434f-ba0f-649dd4e1a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# step-1\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# step-2\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# step-3\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d7bf4e-eaa3-4814-93f5-858c02b6871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step -4\n",
    "vectorstore = FAISS.from_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3748c92d-e407-4ff9-b6e7-537b32872203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18eea66-1886-4a00-ab8f-8998e8d7dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-5\n",
    "#vectorstore.similarity_search(\"what is langchain?\")\n",
    "retriever_obj = vectorstore.as_retriever() # Convert vector stores into a retriever object\n",
    "\n",
    "# Step-6 - create llm object\n",
    "llm_obj = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a6ad76-cdbf-4184-a146-0eb248b52bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-7 - Build Retrieval QA Chain \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef67ff-8d77-4d90-ba07-1eae886edab8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             print(\"OK\")\n",
    "...\n",
    ">>> obj = box()\n",
    "OK\n",
    ">>> callable(obj)\n",
    "False\n",
    ">>>\n",
    ">>> obj()\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "TypeError: 'box' object is not callable\n",
    ">>>\n",
    ">>> def fx():\n",
    "...     print(\"OK\")\n",
    "...\n",
    ">>> type(fx)\n",
    "<class 'function'>\n",
    ">>>\n",
    ">>> callable(fx)\n",
    "True\n",
    ">>> fx()\n",
    "OK\n",
    ">>> class box:\n",
    "...     def __init__(self):\n",
    "...             pass\n",
    "...     def __call__(self):\n",
    "...             print(\"OK\")\n",
    "...\n",
    ">>> obj = box()\n",
    ">>> callable(obj)\n",
    "True\n",
    ">>>\n",
    ">>> fx.__call__()  # same fx()\n",
    "OK\n",
    ">>>\n",
    ">>> obj()\n",
    "OK\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934d79a6-5f77-4a37-bf7b-ba6bf53fe699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_29792\\1664598314.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa_chain(\"what is langchain?\")\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the entire process of building, deploying, and managing LLM-based applications, making it easier to create and optimize these applications.\\n\\nLangChain provides an open-source platform that includes various components and integrations to help developers build stateful agents, monitor and evaluate their applications, and deploy them into production.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-8 QA_Chain ->invoke a query\n",
    "#qa_chain <-- callable object\n",
    "qa_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aeb0be6-efdf-4fc4-bc75-9de83cef0595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write hello world example program in C ?',\n",
       " 'result': 'I don\\'t know how to write a \"Hello World\" example program in C using the context of LangChain, as LangChain seems to be related to large language models and application development, not a programming language like C. However, I can provide a simple \"Hello World\" example program in C:\\n\\n```c\\n#include <stdio.h>\\n\\nint main() {\\n    printf(\"Hello, World!\\\\n\");\\n    return 0;\\n}\\n```\\n\\nTo compile and run this program, you would typically use a C compiler like GCC (GNU Compiler Collection) from the command line:\\n\\n```bash\\ngcc hello.c -o hello\\n./hello\\n```\\n\\nThis will compile the `hello.c` file into an executable named `hello`, and then run the executable, printing \"Hello, World!\" to the console.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(\"How to write hello world example program in C ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47995cf-215b-4a18-a097-ebdc91ab6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karth\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\langchain\\\\__init__.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838a2d65-3df2-4fc3-bc68-a699ede17b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c7edfa-d02d-4f96-9cc3-c7361b6865ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\os.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bc0f61-206f-4957-8e6c-d15baf008a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\requests\\\\__init__.py'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e01098-dfa5-4009-8d83-659bc2fae618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca35d97-c8e8-414e-b3c1-452a5d0b53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompting\n",
    "---------\n",
    " |->prompt  - text \n",
    " |->jinja2 - template code ---> {{variable}} \n",
    "\n",
    " |--> instructing an LLM to give the result/output user wants. //How to talk to AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15495d4f-115c-4808-9f9c-6367108ad4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4601f733-e5bb-401c-8c11-56efe1408855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read java book')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read java book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab826a3-f9fe-49d2-8dc0-9ce0a86ddff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create dynamic value -> template placeholder/variable {variable/placeholder} \n",
    "# PromptTemplate.from_template('user defined prompt') ->object\n",
    "# object.format(userdefined_input_placeholder=Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd6fb4d0-5eb8-4667-b9d9-74da71703ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read html book'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "obj.format(myvar=\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb72f54-8a34-457c-999c-c48810fc1990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read story book'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b48347-5ebc-409b-8280-b1c130f9b939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5236a721-96ed-4ea8-89cd-5cbb3109906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['author', 'my_year', 'myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book written by {author} released on {my_year}')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = PromptTemplate.from_template('I likes to read {myvar} book written by {author} released on {my_year}')\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70e9ed35-8abe-4257-b59d-917bcc7378b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read birds book written by Mr.ABC released on 2004'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.format(myvar=\"birds\",author=\"Mr.ABC\",my_year=2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5f4e0-51be-4929-a1a1-f52215d66211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate\n",
    "------------------\n",
    "|->Langchain utility \n",
    "|->Build structured prompts\n",
    "        ===================\n",
    "               |->Chat bot , QA \n",
    "\n",
    "\n",
    "End_User: Query ---------->--- | Application |  <== Human message - userQuery (or) userinput\n",
    "                                 ..\n",
    "            =======<==========                  <== AI message \n",
    "                Response                                   |<== System message - rules/behavior  \n",
    "\n",
    "\n",
    "1. Role/Context\n",
    "2. Task \n",
    "3. Format - how should the output/result look ?\n",
    "     (ex: provide the answer as a bullet point\n",
    "           write it in json format \n",
    "           keep it under 100 words..)\n",
    "\n",
    "4. Constraints - What rules to follow?\n",
    "      (ex: Don't use technical jargon\n",
    "           use python 3.10+ features)\n",
    "\n",
    "5. Example - optional \n",
    "           \n",
    "List of tuple \n",
    "--------------\n",
    "[(\"system\",define role),(\"human\",userQuery)] \n",
    "\n",
    "Example:\n",
    "[(\"system\",\"You are a helpful AI assistant.\"),\n",
    " (\"human\",\"Explain about {topic} in simple terms\")]\n",
    "|\n",
    "|\n",
    "Langchain Converts into \n",
    "System: \n",
    "Human: \n",
    "\n",
    "UserInput is:  topic=\"transformers\"\n",
    "|\n",
    "|\n",
    "Langchain Converts to\n",
    "|\n",
    "System: You are a helpful AI assistant.\n",
    "Human:  Explain about transformers in simple terms\n",
    "---------------------------------------------------\n",
    " |\n",
    " |_______ sends to chat model\n",
    "\n",
    "system , You are a helpful AI bot. -> SystemMessage(content=\"You are a helpful AI bot.\"....)\n",
    "human, Hello, how are you doing? -> HumanMessage(content=\"Hello, how are you doing?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0c910d-422e-4d5b-9290-333d37c998c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da427f1d-9121-4e77-b2ed-22ec00cceae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc97d6ee-d38a-4927-a5ef-64acef6620a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python books'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read python books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7de84653-75d6-428a-abbc-36c04d54a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['mybook'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['mybook'], input_types={}, partial_variables={}, template='I likes to read {mybook} books'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "844ecd52-2a62-4d86-a4a1-9803dcc2c4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read ruby books', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books')\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(mybook=\"ruby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814a2cc-3cf0-4fc8-87d9-903d7520d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.invoke(\"what is langchain?\") --->HumanMessage(content=\"what is langchain?\",.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1792a0c4-f9e4-4d10-bb51-025e0177b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read ruby books', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books'),\n",
    "        ('ai','Sure')\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(mybook=\"ruby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5092d5a2-e08e-4d5b-a1f6-cfa4bb9c0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_29792\\275367955.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  my_llm_obj = Ollama(model=\"gemma2:2b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "my_llm_obj = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7153b19c-ed2f-4bd9-a073-a8392efdee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84809a11-cb0b-42b3-8a87-873504aa8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "197c4e60-fbdd-409b-b51b-9b12225d279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GenAI, or **Generative Artificial Intelligence**, is a type of artificial intelligence that focuses on creating new content.  \\n\\nHere's a breakdown:\\n\\n**What it does:**\\n\\n* **Creates original work**: This can range from text and images to music, code, videos, and even 3D models.\\n* **Learns patterns and relationships**: GenAI uses vast datasets of information to understand how things are connected and generate new content based on learned patterns.  \\n* **Applies creative techniques**: It can emulate human creativity by producing diverse, original output in various styles and formats.\\n\\n**How it works:**\\n\\nGenAI usually relies on powerful algorithms that process massive amounts of data to train models like:\\n\\n* **Language Models**: GPT-3, Bard, etc. excel at understanding and generating human-like text.\\n* **Image Models**: DALL-E 2, Midjourney, etc. can create realistic or imaginative images from text prompts. \\n\\n**Examples:**\\n\\n* **Writing Assistant**:  Tools like Jasper or Copy.ai help users generate content quickly and efficiently for writing blog posts, social media updates, etc.\\n* **Image Generation**: Artists are using AI tools to design unique artwork, while marketing teams use them to create custom visuals for campaigns.\\n* **Code Generation**: Tools like GitHub Copilot can suggest code snippets based on user prompts, accelerating the development process. \\n\\n**Overall:** GenAI is revolutionizing content creation and opening up new possibilities in various industries!\\n\\n\\nLet me know if you have any more questions about GenAI!  ðŸ˜Š \\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e404909-09e7-48f4-9c78-960deedc5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Say don't know\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acec117a-5fb7-4f8f-a7f2-e1b5b3747d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = chat_prompt|my_llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1105b21a-be81-40fd-8b19-9cada9560e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Say: Don't know \\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5da6c63-1548-4feb-a922-8e51aae3551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your AI assistant\"),\n",
    "    (\"user\",\"Question:{question}\")\n",
    "])\n",
    "my_chain = chat_prompt|llm_obj # groq object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb71006e-23d5-4725-a98a-431c773d3da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='GenAI, short for Generalized Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, reason, and apply its knowledge across a wide range of tasks, similar to human intelligence.\\n\\nGenAI is often considered a type of artificial general intelligence (AGI), which is a more specific term that refers to an AI system that has the ability to understand, learn, and apply its knowledge to solve any problem, in the same way that a human would.\\n\\nThe key characteristics of GenAI include:\\n\\n1. **General knowledge**: Ability to understand and apply knowledge across various domains and tasks.\\n2. **Reasoning and problem-solving**: Ability to reason, solve problems, and make decisions, like a human would.\\n3. **Learning and adaptation**: Ability to learn from experience, adapt to new situations, and improve over time.\\n4. **Self-awareness and consciousness**: Some researchers believe that GenAI may also possess self-awareness and consciousness, although this is still a topic of debate.\\n\\nGenAI is considered a significant milestone in the development of artificial intelligence, as it would enable AI systems to:\\n\\n1. **Assist humans**: GenAI could assist humans in various tasks, such as healthcare, finance, education, and more.\\n2. **Create new technologies**: GenAI could generate new ideas, products, and services that would transform industries and society.\\n3. **Improve decision-making**: GenAI could provide humans with more accurate and informed decision-making capabilities.\\n\\nHowever, the development of GenAI also raises significant concerns, such as:\\n\\n1. **Job displacement**: GenAI could automate many jobs, potentially leading to significant job displacement.\\n2. **Bias and ethics**: GenAI could perpetuate biases and make decisions that are unfair or unethical.\\n3. **Security risks**: GenAI could pose significant security risks if not designed and developed with robust safety protocols.\\n\\nWhile researchers have made significant progress in developing narrow AI systems, the development of GenAI remains a subject of ongoing research and debate.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 45, 'total_tokens': 452, 'completion_time': 0.536295079, 'prompt_time': 0.00215373, 'queue_time': 0.050621596, 'total_time': 0.538448809}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--87056912-0d30-42b6-a48b-72dffe6d3059-0', usage_metadata={'input_tokens': 45, 'output_tokens': 407, 'total_tokens': 452})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'what is genAI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3530b3ce-db2b-4e92-9efa-b78b9971d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Your sql expert\"),\n",
    "    (\"user\",\"Get list of users from {question} dept\"),\n",
    "    (\"ai\",\"select *from users\")\n",
    "])\n",
    "my_chain = chat_prompt|llm_obj # groq object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "418c8cdb-15da-41e6-841b-1479e6c8444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" \\nwhere department = 'Sales'\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56, 'completion_time': 0.02777622, 'prompt_time': 0.002276861, 'queue_time': 0.050675056, 'total_time': 0.030053081}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7b291980-26c0-4d24-8e5d-af039b4ea891-0', usage_metadata={'input_tokens': 48, 'output_tokens': 8, 'total_tokens': 56})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chain.invoke({'question':'sales'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897f592-b42b-4b09-bc59-0b3dc406d665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45811b53-03df-4057-972d-18b86b10844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization\n",
    "text = '''\n",
    "Aritfical Intelligence(AI) is transforming industries..\n",
    "AI models are used in hearthcare,finance,education,enterprise indus and also supports autonomous system'''\n",
    "\n",
    "prompt = f'Summarize the following text in 3 bullet points:\\n{text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "119f2cb9-93ca-4985-8acf-e11264f5e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 3-bullet point summary of the text:\n",
      "\n",
      "* **Artificial Intelligence (AI) is impacting diverse industries.** AI models are being utilized in fields such as healthcare, finance, education, enterprise operations, and even support for autonomous systems.\n",
      "* **AI's influence extends across various sectors.**  The text highlights the widespread application of AI across different industries, showcasing its versatility and reach. \n",
      "* **Key applications include healthcare, finance, and education.** The text specifically mentions how AI is used in these areas as examples of the impact of AI on daily life. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_llm_obj.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa2682-5818-4e60-815e-f59ca5d2081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''Summarize the following text in 3 bullet points:\\n{text}\n",
    "           1. data1\n",
    "           2. data2\n",
    "           3. data3'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b996e-e2e4-4f07-8f2d-0315ecbfcad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334a28c-1366-4838-b719-7ea0b2227fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "# Grammar and Text Correction\n",
    "# Text to structured \n",
    "# Code generation\n",
    "# Content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bab4d6d-be4b-47f9-8761-426530d14be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s the corrected text:\\n\\n\"AI is changing the world in many ways. It helps people to work faster.\"\\n\\nHere\\'s a breakdown of the corrections:\\n\\n- \"AI are\" should be \"AI is\" (subject-verb agreement: AI is a singular subject, so it should use a singular verb)\\n- \"chaining\" should be \"changing\" (correct spelling)\\n- \"the world in many way\" should be \"the world in many ways\" (plural form of \"way\")\\n- \"It help\" should be \"It helps\" (subject-verb agreement: a singular subject \"it\" should use a singular verb \"helps\")\\n- \"peoples\" should be \"people\" (correct spelling)' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 60, 'total_tokens': 207, 'completion_time': 0.209621642, 'prompt_time': 0.00295258, 'queue_time': 0.06125126, 'total_time': 0.212574222}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--3057fa92-f402-4bca-8451-a2e7c50791dc-0' usage_metadata={'input_tokens': 60, 'output_tokens': 147, 'total_tokens': 207}\n"
     ]
    }
   ],
   "source": [
    "# Grammar and Text Correction\n",
    "prompt = '''\n",
    "Correct grammar and spelling mistakes in this text:\n",
    "\"AI are chaining the world in many way.It help peoples to work faster\"\n",
    "'''\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25c3ba4e-7f92-445d-bffc-c57ea2f6c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the corrected text:\n",
      "\n",
      "\"AI is changing the world in many ways. It helps people to work faster.\"\n",
      "\n",
      "Here's a breakdown of the corrections:\n",
      "\n",
      "- \"AI are\" should be \"AI is\" (subject-verb agreement: AI is a singular subject, so it should use a singular verb)\n",
      "- \"chaining\" should be \"changing\" (correct spelling)\n",
      "- \"the world in many way\" should be \"the world in many ways\" (plural form of \"way\")\n",
      "- \"It help\" should be \"It helps\" (subject-verb agreement: a singular subject \"it\" should use a singular verb \"helps\")\n",
      "- \"peoples\" should be \"people\" (correct spelling)\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "262056ff-8e42-4921-9d6b-b2f14606a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 bullet points summarizing the text:\n",
      "\n",
      "â€¢ LangChain is an open-source framework for developing applications powered by large language models (LLMs).\n",
      "â€¢ The framework simplifies the entire lifecycle of LLM application development, including development, productionization, and deployment.\n",
      "â€¢ During development, LangChain provides open-source components and third-party integrations to build and customize applications.\n",
      "â€¢ LangSmith is used for productionization, allowing developers to inspect, monitor, and evaluate applications for continuous optimization and deployment.\n",
      "â€¢ LangChain's LangGraph Platform enables the deployment of LangGraph applications as production-ready APIs and Assistants.\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "# Task \n",
    "# read my_docs.txt file - use python file handling - open a inputfile - read a content - pass this input to prompt\n",
    "# define prompt - Summarize 5 bullet points \n",
    "\n",
    "s  = open('my_docs.txt').read()\n",
    "\n",
    "prompt = f'Summarize the following text in 5 bullet points:\\n\\n{s}'\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbf7d703-9701-4e6e-80f5-147ae0bf2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see any given text. Please provide the text, and I will extract the information and present it in JSON format.\n",
      "\n",
      "However, if you provide the text \"Ctpl indus was founded by Mr.Ram in year 1982 in Bangalore\", here's the extracted information in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"Company Name\": \"Ctpl indus\",\n",
      "    \"Founder\": \"Mr. Ram\",\n",
      "    \"Year Founded\": 1982,\n",
      "    \"Location\": \"Bangalore\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Text to structured \n",
    "text = \"Ctpl indus was founded by Mr.Ram in year 1982 in Bangalore\"\n",
    "\n",
    "prompt = f'From the given text:{text} extract information in json'\n",
    "\n",
    "response = llm_obj.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0a09686-a0d8-44cd-b361-40fb3be618f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Factorial of a Number in Python**\n",
      "====================================================\n",
      "\n",
      "Here's a simple Python function that calculates the factorial of a given number:\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number for which the factorial is to be calculated.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of the given number.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input number is negative.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input number must be non-negative.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * calculate_factorial(n-1)\n",
      "```\n",
      "**Example Use Cases**\n",
      "--------------------\n",
      "\n",
      "```python\n",
      "# Calculate the factorial of 5\n",
      "print(calculate_factorial(5))  # Output: 120\n",
      "\n",
      "# Calculate the factorial of 0\n",
      "print(calculate_factorial(0))  # Output: 1\n",
      "\n",
      "# Calculate the factorial of a negative number (raises ValueError)\n",
      "try:\n",
      "    print(calculate_factorial(-5))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Input number must be non-negative.\n",
      "\n",
      "# Calculate the factorial of a non-integer (raises TypeError)\n",
      "try:\n",
      "    print(calculate_factorial(3.5))\n",
      "except TypeError as e:\n",
      "    print(e)  # Output: Input must be an integer.\n",
      "```\n",
      "**Alternative Recursive Implementation**\n",
      "--------------------------------------\n",
      "\n",
      "For a more efficient recursive implementation, you can use memoization to store the factorial of previously calculated numbers:\n",
      "```python\n",
      "factorial_cache = {0: 1, 1: 1}\n",
      "\n",
      "def calculate_factorial(n):\n",
      "    if n not in factorial_cache:\n",
      "        factorial_cache[n] = n * calculate_factorial(n-1)\n",
      "    return factorial_cache[n]\n",
      "```\n",
      "This approach avoids redundant calculations and improves performance for large inputs. However, it uses more memory to store the cache.\n",
      "\n",
      "**Iterative Implementation**\n",
      "---------------------------\n",
      "\n",
      "For an iterative approach, you can use a simple loop to calculate the factorial:\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input number must be non-negative.\")\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "This implementation is more efficient than the recursive approach and does not use any extra memory.\n"
     ]
    }
   ],
   "source": [
    "# # Code generation\n",
    "\n",
    "input_ = 'Write a python function to calculate factorial of a number'\n",
    "response = llm_obj.invoke(input_)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1ca79a7-782d-496b-b8d0-f949de928f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the text:\n",
      "\n",
      "\"Our product is affordable and useful for every customer, whether in a formal or professional setting.\"\n"
     ]
    }
   ],
   "source": [
    "# Text -->Rewrite \n",
    "text = \"Our product is afforable and useful for every customers\"\n",
    "\n",
    "prompt = f\"Rewrite the give text:{text} in formal and profession:\"\n",
    "\n",
    "print(llm_obj.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3113a6df-f3a3-404e-930e-a0a140147881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample email for a professional appointment notification:\n",
      "\n",
      "Subject: Job Offer and Appointment Notification\n",
      "\n",
      "Dear [Candidate Name],\n",
      "\n",
      "We are pleased to inform you that after a thorough evaluation of your application and interview process, we are delighted to offer you the position of [Job Title] at [Company Name]. Your exceptional skills, experience, and passion for the field made you stand out among other candidates, and we believe you would be a valuable addition to our team.\n",
      "\n",
      "Below are the details of your appointment:\n",
      "\n",
      "- Job Title: [Job Title]\n",
      "- Department: [Department Name]\n",
      "- Reporting Manager: [Manager's Name]\n",
      "- Start Date: [Start Date]\n",
      "- Salary: [Salary Amount]\n",
      "- Benefits: [List of benefits, if applicable]\n",
      "\n",
      "We have attached a copy of the employment contract, which outlines the terms and conditions of your appointment. Please review it carefully and do not hesitate to reach out to us if you have any questions or concerns.\n",
      "\n",
      "To confirm your acceptance, please sign and return a copy of the contract to us by [Deadline for response]. We also require a start date confirmation by [Deadline for response].\n",
      "\n",
      "We look forward to welcoming you to our team and working together to achieve great things. If you have any questions or need further information, please do not hesitate to contact us.\n",
      "\n",
      "Congratulations once again on your appointment, and we look forward to hearing back from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Title]\n",
      "[Company Name]\n",
      "[Contact Information]\n",
      "\n",
      "Attachments: \n",
      "- Employment Contract\n",
      "- Job Description\n",
      "- Company Policies and Procedures (if applicable)\n",
      "\n",
      "Please note that you should customize this email to fit your company's specific needs and requirements.\n"
     ]
    }
   ],
   "source": [
    "input_var = 'Give me professional appointment email for a candidate who was selected'\n",
    "print(llm_obj.invoke(input_var).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2b36a-9528-4756-8233-198c1dd0dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Internal \n",
    "|->prompt<--->llm\n",
    "\n",
    "input prompt ->tokenizaion \n",
    "                       |->tokenID --->embedding vector(768/1024/4096)\n",
    "\n",
    "prompt: \"Explain how to write C program\" \n",
    "\n",
    "Tokenization --> ['Explain','how','to','write','C','program']\n",
    "                     |        |     |    |      |    |\n",
    "\"Explain\" -->[0.89,0.56,-0.76...]\n",
    "\n",
    "Transformer - Positional encoding\n",
    "------------\n",
    " |->self-attention\n",
    "    |\n",
    "   attention score\n",
    "Feed-Forward Network\n",
    "\n",
    "Prompt-Text\n",
    "|\n",
    "Prompt-Token\n",
    "|\n",
    "embedding+position\n",
    "|\n",
    "TF-1\n",
    "|\n",
    "TF-2\n",
    "..\n",
    "|\n",
    "softmax - probability of next token\n",
    "|->select next token\n",
    "|\n",
    "Append token -->repeat\n",
    "\n",
    "Prompt rule                     Internal\n",
    "You are an expert   ------>  Activates expert-style pattern\n",
    "Step by step       ---------> Raises probability of chain-like structure\n",
    "...\n",
    "use json --------------> strong - json format\n",
    "\n",
    "Do not explain ------------ suppresses explianation tokens\n",
    "|\n",
    "Repetition ----------------- Increases attention weight\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfdeef-40d1-49ab-901b-f225599787f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompting Techniques\n",
    "\n",
    "shot -- example\n",
    "1. Zero-shot (no example)\n",
    "==========================\n",
    "prompt: \"Translate this to french: I like to write program\"\n",
    "|\n",
    "response : j\"adore....\"\n",
    "\n",
    "2.one-shot prompt \n",
    "prompt: \"\"\"Translate this to french:\n",
    "           ex: hello -> bsdfsfd\" \n",
    "           I like to write program\"\"\"\n",
    "reponse: ...\n",
    "\n",
    "few-shot prompt\n",
    "----------------\n",
    "Text:\" i like this product\" ->positive\n",
    "text: \" i not like this product\" ->negative\n",
    "text: \" it's ok\" ->neutral\n",
    "\n",
    "Text: \"The product is good\" -> \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ab99b08-1830-4d6b-9f3f-160a2aa0b184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I likes to read ruby books', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Continue from ChatPromptTemplate\n",
    "##\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are helpful assistant'),\n",
    "        ('human','I likes to read {mybook} books')\n",
    "    ]\n",
    ")\n",
    "prompt.format_messages(mybook=\"ruby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bee61b11-993a-4ad9-a01d-aebf7be7efd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "HumanMessage(content=\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f19d1fa-9518-4c50-9822-09be66050406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langchain is an AI technology company that focuses on developing a conversational AI platform that allows users to have more human-like conversations with AI models. The company uses a combination of natural language processing (NLP), machine learning, and knowledge retrieval to create a platform that can understand complex queries and provide relevant and accurate responses.\\n\\nLangchain\\'s platform is designed to be highly flexible and scalable, allowing it to handle a wide range of tasks and applications, from customer service and support to content generation and research. The company\\'s technology is based on a concept called \"conversational knowledge retrieval,\" which involves using a large dataset of text to train AI models to retrieve and integrate relevant information from multiple sources.\\n\\nSome of the key features of Langchain\\'s platform include:\\n\\n1. **Conversational interface**: Langchain\\'s platform provides a conversational interface that allows users to interact with AI models in a natural and intuitive way.\\n2. **Knowledge retrieval**: The platform uses a large dataset of text to train AI models to retrieve and integrate relevant information from multiple sources.\\n3. **Integration with other tools**: Langchain\\'s platform can be integrated with other tools and systems, such as CRM systems and customer service platforms.\\n4. **Scalability**: The platform is designed to be highly scalable, allowing it to handle a large volume of conversations and interactions.\\n\\nLangchain has gained significant attention in the AI and tech industries due to its innovative approach to conversational AI and its potential applications in various fields, such as customer service, marketing, and research.\\n\\nIn 2023, Langchain announced a new product called LLaMA (Large Language Model Application) which is a large language model developed by Meta AI. It is a multimodal large language model that can process and generate human-like text, images, and other forms of media.\\n\\nHowever, there is another company called LangChain Labs which is a part of the broader LangChain ecosystem. LangChain Labs is focused on developing and providing open-source tools and infrastructure for building and deploying conversational AI models. Their primary product is called the LangChain Core, which is a modular and extensible architecture for building conversational AI applications.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 435, 'prompt_tokens': 40, 'total_tokens': 475, 'completion_time': 0.706510775, 'prompt_time': 0.002428864, 'queue_time': 0.063073107, 'total_time': 0.708939639}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0bf92cd6-f910-4c39-a0d5-600b61d151e5-0', usage_metadata={'input_tokens': 40, 'output_tokens': 435, 'total_tokens': 475})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_obj.invoke(\"What is langchain?\") \n",
    "llm_obj.invoke([HumanMessage(content=\"What is Langchain?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9246963-a8e5-4163-9f29-5b35d3036beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Karthik, nice to meet you. As an instructor, what subject or field do you teach? And what do you need assistance with today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 45, 'total_tokens': 77, 'completion_time': 0.053060352, 'prompt_time': 0.002185185, 'queue_time': 0.058014864, 'total_time': 0.055245537}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3765094f-3138-41ac-8c31-748ed312d67b-0', usage_metadata={'input_tokens': 45, 'output_tokens': 32, 'total_tokens': 77})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke([HumanMessage(content=\"Hello my name is Karthik I am instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5055da93-e143-4ccc-8672-9678a872bd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have information about your name. I'm a large language model, I don't have personal connections or access to personal data. Each time you interact with me, it's a new conversation, and I don't retain any information from previous conversations. If you'd like to share your name with me, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 41, 'total_tokens': 114, 'completion_time': 0.083559212, 'prompt_time': 0.002989094, 'queue_time': 0.057192535, 'total_time': 0.086548306}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--61628873-1dcf-4ab8-9dd0-9007fc9c1b32-0', usage_metadata={'input_tokens': 41, 'output_tokens': 73, 'total_tokens': 114})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj.invoke([HumanMessage(content=\"Hi Tell me my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a9876-3d3c-4708-ad82-620abebb5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_Q: what is python?\n",
    "AI_R:   ............\n",
    "User_Q: list out any 5 important features\n",
    "|\n",
    "AI_R: there are 5 important python features\n",
    "|\n",
    "\n",
    "-----------\n",
    "User: Hello my name is Tom ...\n",
    "AI: Hello Tom ... my name is userA ....\n",
    "user: renewal my website domain\n",
    "AI: ... type your doamin\n",
    "User: \n",
    " ---------------------------------------//Session expired \n",
    "\n",
    "user: I will type website name?\n",
    "AI : May i know your name...\n",
    " -----------------------------------------//new session\n",
    "\n",
    "  +--------------------------------------------+\n",
    "  | SessionID     |    Chat(Human+AI)          |\n",
    "  |---------------|----------------------------|\n",
    "  |  Session1     | User: ....\n",
    "  |               | AI_R:\n",
    "  |               | User:\n",
    "  |               | AI_R:\n",
    "  |               | ..\n",
    "  |---------------|-----------------------------\n",
    "  |  session2     | User: Hello my name is Tom\n",
    "  |               | AI : Hello Tom ..\n",
    "  |               | ...\n",
    "  |               | User: \"Hi Tell me my name?\"\n",
    "  |               | AI : Hello Tom ..\n",
    " +--------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6134afe3-d80a-4dda-a08e-42cb5048c11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p101': 'pA', 'p102': 'pB', 'p103': 'pC'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = {}\n",
    "stores['p101'] = 'pA'\n",
    "stores['p102'] = 'pB'\n",
    "stores['p103'] = 'pC'\n",
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b586ad68-1c62-475a-93b6-a68505c85ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p101': ['pA', 1000, 'pVendor1'],\n",
       " 'p102': ['pB', 2000, 'pVendor2'],\n",
       " 'p103': ['pC', 3000, 'pVendor3']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = {}\n",
    "stores['p101'] = ['pA',1000,'pVendor1']\n",
    "stores['p102'] = ['pB',2000,'pVendor2']\n",
    "stores\n",
    "if('p103' not in stores):\n",
    "    stores['p103'] = ['pC',3000,'pVendor3']\n",
    "\n",
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7ca68c0-4127-46a1-9e09-12fd8e232ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pB', 2000, 'pVendor2']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['p102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d38dcf-92cd-41fb-950c-418f42ad81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatMessageHistory --- stores message(Human + AI)\n",
    "|\n",
    "BaseChatMessageHistory --- Abstractbase class - interface \n",
    "|\n",
    "RunnableWithMessageHistory - Langchain wrapper - combine an LLM messagehistory\n",
    "\n",
    "\n",
    "def userdefinedFunction(session_id) ->BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "\n",
    "obj = RunnableWithMessageHistory(llm_object,userdefinedFunction)\n",
    "obj.invoke('message':[],config={'sessionID':<....>'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e832e334-c12b-4cae-b47e-e18d0ed0eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9ec29c9-6271-4a51-9005-d03995f56776",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = {} \n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "\n",
    "obj = RunnableWithMessageHistory(llm_obj,get_session_history)\n",
    "\n",
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4105d37-155a-42d2-99e1-5df0f39fbf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 45, 'total_tokens': 74, 'completion_time': 0.043072448, 'prompt_time': 0.002135029, 'queue_time': 0.050844168, 'total_time': 0.045207477}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--d7673ce8-afb8-4b52-9cea-75d02ec876b3-0' usage_metadata={'input_tokens': 45, 'output_tokens': 29, 'total_tokens': 74}\n"
     ]
    }
   ],
   "source": [
    "response = obj.invoke([HumanMessage(content=\"Hello my name is Karthik I am instructor\")],config=my_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9bb65cd1-df46-4cfb-b529-9f9a09b8702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3975ebc-dca0-4ab4-90b6-81f116fe46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about your personal identity, including your name. I'm a large language model, I interact with many users, and each conversation is a new, anonymous interaction. If you'd like to share your name, I can use it in our conversation.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = obj.invoke([HumanMessage(content=\"What is my name?\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a9040a9-ab62-49eb-8541-80330e8dfb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Karthik.\n"
     ]
    }
   ],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response = obj.invoke([HumanMessage(content=\"What is my name?\")],config=my_config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1df17c22-6c4e-44bd-8a86-e1a30a016e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello my name is Karthik I am instructor', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 45, 'total_tokens': 74, 'completion_time': 0.043072448, 'prompt_time': 0.002135029, 'queue_time': 0.050844168, 'total_time': 0.045207477}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7673ce8-afb8-4b52-9cea-75d02ec876b3-0', usage_metadata={'input_tokens': 45, 'output_tokens': 29, 'total_tokens': 74}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Karthik.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 88, 'total_tokens': 96, 'completion_time': 0.01345187, 'prompt_time': 0.005041959, 'queue_time': 0.050036771, 'total_time': 0.018493829}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--928a251e-4b1c-4502-bd29-38a026ebf692-0', usage_metadata={'input_tokens': 88, 'output_tokens': 8, 'total_tokens': 96})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have any information about your personal identity, including your name. I'm a large language model, I interact with many users, and each conversation is a new, anonymous interaction. If you'd like to share your name, I can use it in our conversation.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 40, 'total_tokens': 96, 'completion_time': 0.071706416, 'prompt_time': 0.002479665, 'queue_time': 0.051354455, 'total_time': 0.074186081}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--626c385b-42a1-4667-8160-df9edac9d9d9-0', usage_metadata={'input_tokens': 40, 'output_tokens': 56, 'total_tokens': 96})])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2652fb9e-8711-4dfb-9c69-f66d4304d09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello my name is Karthik I am instructor', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 45, 'total_tokens': 74, 'completion_time': 0.043072448, 'prompt_time': 0.002135029, 'queue_time': 0.050844168, 'total_time': 0.045207477}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7673ce8-afb8-4b52-9cea-75d02ec876b3-0', usage_metadata={'input_tokens': 45, 'output_tokens': 29, 'total_tokens': 74}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Karthik.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 88, 'total_tokens': 96, 'completion_time': 0.01345187, 'prompt_time': 0.005041959, 'queue_time': 0.050036771, 'total_time': 0.018493829}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--928a251e-4b1c-4502-bd29-38a026ebf692-0', usage_metadata={'input_tokens': 88, 'output_tokens': 8, 'total_tokens': 96})])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['chat1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67968a01-a51f-4dc4-966c-36e967f78f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello my name is Karthik I am instructor', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 45, 'total_tokens': 74, 'completion_time': 0.043072448, 'prompt_time': 0.002135029, 'queue_time': 0.050844168, 'total_time': 0.045207477}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7673ce8-afb8-4b52-9cea-75d02ec876b3-0', usage_metadata={'input_tokens': 45, 'output_tokens': 29, 'total_tokens': 74}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Karthik.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 88, 'total_tokens': 96, 'completion_time': 0.01345187, 'prompt_time': 0.005041959, 'queue_time': 0.050036771, 'total_time': 0.018493829}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--928a251e-4b1c-4502-bd29-38a026ebf692-0', usage_metadata={'input_tokens': 88, 'output_tokens': 8, 'total_tokens': 96})])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(stores['chat1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fecc414-7b7b-4dbe-b3b2-51e848a4aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e431cd2-abec-4070-9c31-87e76c73075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = obj.invoke([HumanMessage(content=\"what is langchain?\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1bda065-dab3-40f7-805e-e8b3c66904f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24e535fa-f645-4459-8832-fe6813cbe1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chat1', 'chat2', 'chat3'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a2749a7-5b6a-4ad8-90df-bf0c6cb9c680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='what is langchain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is an open-source framework for building large language models (LLMs) applications. It was created by Joshua Browning, the founder of LangChain, and is designed to make it easier for developers to build applications that utilize LLMs.\\n\\nLangChain provides a set of tools and libraries that enable developers to:\\n\\n1. **Integrate LLMs**: LangChain supports a wide range of LLMs, including popular models like LLaMA, BERT, and RoBERTa.\\n2. **Build custom models**: Developers can use LangChain to create custom LLMs that are tailored to their specific use cases.\\n3. **Implement natural language processing (NLP) tasks**: LangChain includes a range of NLP tasks, such as text classification, sentiment analysis, and named entity recognition.\\n4. **Create interactive applications**: LangChain makes it easy to build interactive applications that involve LLMs, such as chatbots and virtual assistants.\\n\\nSome of the key features of LangChain include:\\n\\n1. **Model-agnostic architecture**: LangChain's architecture is designed to be model-agnostic, meaning that it can support a wide range of LLMs without requiring significant changes to the code.\\n2. **Modular design**: LangChain is built on a modular design, which makes it easy to add new features and functionality without modifying the core code.\\n3. **Flexibility**: LangChain provides a high degree of flexibility, allowing developers to customize the framework to suit their specific needs.\\n\\nLangChain is particularly well-suited for building applications that require the use of LLMs, such as:\\n\\n1. **Chatbots**: LangChain makes it easy to build chatbots that can engage in natural-sounding conversations.\\n2. **Virtual assistants**: LangChain can be used to build virtual assistants that can perform a wide range of tasks, from scheduling appointments to answering questions.\\n3. **Content generation**: LangChain can be used to generate high-quality content, such as articles, social media posts, and product descriptions.\\n\\nOverall, LangChain is a powerful framework that makes it easier for developers to build applications that utilize LLMs. Its modular design, flexibility, and model-agnostic architecture make it an attractive choice for developers who want to create innovative applications that leverage the power of LLMs.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 465, 'prompt_tokens': 40, 'total_tokens': 505, 'completion_time': 0.573064184, 'prompt_time': 0.001809644, 'queue_time': 0.052425096, 'total_time': 0.574873828}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8c16cfc3-f4ae-4935-bdcd-e7b6e1fcd6d6-0', usage_metadata={'input_tokens': 40, 'output_tokens': 465, 'total_tokens': 505})])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['chat3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5d8de1b-7a04-4e8e-94c3-df83ed4e6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response = obj.invoke([HumanMessage(content=\"Explain about genAI in simple term in oneline?\")],config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "450a838b-a6a9-40a5-8843-f63b3b55d21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello my name is Karthik I am instructor', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Karthik, nice to meet you. You're an instructor, that's great. What subject or field do you instruct in?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 45, 'total_tokens': 74, 'completion_time': 0.043072448, 'prompt_time': 0.002135029, 'queue_time': 0.050844168, 'total_time': 0.045207477}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7673ce8-afb8-4b52-9cea-75d02ec876b3-0', usage_metadata={'input_tokens': 45, 'output_tokens': 29, 'total_tokens': 74}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Karthik.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 88, 'total_tokens': 96, 'completion_time': 0.01345187, 'prompt_time': 0.005041959, 'queue_time': 0.050036771, 'total_time': 0.018493829}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--928a251e-4b1c-4502-bd29-38a026ebf692-0', usage_metadata={'input_tokens': 88, 'output_tokens': 8, 'total_tokens': 96}), HumanMessage(content='Explain about genAI in simple term in oneline?', additional_kwargs={}, response_metadata={}), AIMessage(content='GenAI (Generative AI) is a type of AI that can create new and original content, such as text, images, music, or videos, using patterns and structures learned from existing data.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 117, 'total_tokens': 158, 'completion_time': 0.055571861, 'prompt_time': 0.007151062, 'queue_time': 0.049181266, 'total_time': 0.062722923}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9fd91b7b-b5d3-45d3-af55-203298d594e7-0', usage_metadata={'input_tokens': 117, 'output_tokens': 41, 'total_tokens': 158})])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['chat1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f90f0ff-1bd8-49d9-90b2-23a73652c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "with open('chat_history.log','a') as wobj:\n",
    "    for var in stores:\n",
    "        wobj.write(var+str(stores[var])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0808c-b373-486e-ae90-aa4b769a88fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f552d27-d3da-4c3c-be51-a78472bd1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recap - PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c21b34-5210-46ad-940e-7bc9290fd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompting Techniques\n",
    "\n",
    "shot -- example\n",
    "1. Zero-shot (no example)\n",
    "==========================\n",
    "prompt: \"Translate this to french: I like to write program\"\n",
    "|\n",
    "response : j\"adore....\"\n",
    "\n",
    "2.one-shot prompt \n",
    "prompt: \"\"\"Translate this to french:\n",
    "           ex: hello -> bsdfsfd\" \n",
    "           I like to write program\"\"\"\n",
    "reponse: ...\n",
    "\n",
    "few-shot prompt\n",
    "----------------\n",
    "Text:\" i like this product\" ->positive\n",
    "text: \" i not like this product\" ->negative\n",
    "text: \" it's ok\" ->neutral\n",
    "\n",
    "Text: \"The product is good\" -> \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19818c86-3df9-483c-8d74-f4c5d0a38536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "750890e2-bc67-41e3-a1e7-32565e445b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = ChatPromptTemplate.from_template('''\n",
    "classify the sentiment of the given text as postitive,negative or neutral\n",
    "text:{input}\n",
    "Sentiment:''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4999b695-d6b1-4036-8e37-2e56295ae3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sentiment: Positive', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 59, 'total_tokens': 64, 'completion_time': 0.009624349, 'prompt_time': 0.003507781, 'queue_time': 0.052306389, 'total_time': 0.01313213}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4ba19234-226b-4abe-a214-9c0472bb5fed-0', usage_metadata={'input_tokens': 59, 'output_tokens': 5, 'total_tokens': 64})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"I like this product\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2a7bc8f-9be9-4bcd-b0e3-c042f5a00889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sentiment: Negative', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 60, 'total_tokens': 65, 'completion_time': 0.009130335, 'prompt_time': 0.004542468, 'queue_time': 0.050857482, 'total_time': 0.013672803}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1d602d06-619e-4bcb-9703-353ebef6c835-0', usage_metadata={'input_tokens': 60, 'output_tokens': 5, 'total_tokens': 65})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"I not like this product\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2660add-4f07-437f-9491-15112cca2029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sentiment of the given text is: Neutral.\\n\\nThe word \"ok\" is a neutral expression that does not convey strong emotions. It simply indicates a lack of strong positive or negative feelings, making the overall sentiment of the text neutral.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 60, 'total_tokens': 108, 'completion_time': 0.067512557, 'prompt_time': 0.003119147, 'queue_time': 0.053844503, 'total_time': 0.070631704}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--80c3b128-6006-47cd-b5a4-5366743957b3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 48, 'total_tokens': 108})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot = zero_shot_prompt.format_messages(input=\"the product is ok\")\n",
    "llm_obj.invoke(zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8b1f6-5051-4b78-97f0-be1d1075e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Translate the following english sentence to french\n",
    "French: \"......\"  <== example is given inthis prompt - 1example - 1shot\n",
    "Now translate:\n",
    "English: \"I am learning genAI\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c9438-2f91-4586-b053-11520cb55ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8493fc6-cc73-4125-9552-c5a010b11b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict RAG ===> Answer from loaded document only -- Avoid hullucination \n",
    "                     -----------------------\n",
    "                        {context} \n",
    "\n",
    "context - retrived_chunks \n",
    "--------\n",
    "\n",
    "prompt = f'''\n",
    "Answer the question based only on the following  context:\n",
    "context: {context}\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [],\n",
    "    template=template_obj\n",
    ")\n",
    "\n",
    "prompt.format() ->formatted_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34f71c77-8652-42b1-a1a2-651b76c8b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA # Higher version of langchain -> from langchain_classic.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6d9e7e7-ffb6-4676-a93f-5dabf4b9a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 200\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step 1 Load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# step 2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# step 3 embedding object\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step 4 stores to vectordb\n",
    "vectorstore  = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# step 5 create retrievalobject\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 create llm object\n",
    "llm_obj = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# step 7 prompt - Strict prompt - Strict RAG\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a strict retrieval QA assistant.\n",
    "    Answer only using the information given in the context below.\n",
    "    if the answer is not present in the context, reply exactly with:\n",
    "    \"I don't know, the document doesnot contain this information.\"\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# step 8: QAChain\n",
    "rag_chain = RetrievalQA.from_chain_type(llm = llm_obj,retriever=retriever_obj,chain_type_kwargs={\"prompt\":my_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3e2571e-d3e9-4d6c-94ee-be8a152d801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is lanchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs).'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20129c77-f7f1-4d92-9d4a-c81bbacc5d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How to write Hello world example in C?',\n",
       " 'result': \"I don't know, the document does not contain this information.\"}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"How to write Hello world example in C?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "662aeaef-a256-46a7-ab79-25c253b4cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is langchain?',\n",
       " 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs).'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdadc7-e7f5-43c8-a1a3-7011e61c1b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df794cfb-82c6-45c5-a9b1-2491debfbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is factorial?',\n",
       " 'result': \"I don't know, the document does not contain this information.\"}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"what is factorial?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3a1b0-e2e1-48d6-9242-2195aff1d1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e021b378-bc8b-4913-90b2-954bce525b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "908b4076-c2bb-4930-a9ba-9ae7743452cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 200\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# step 1 Load the data\n",
    "loader = TextLoader('my_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# step 2 split - chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# step 3 embedding object\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step 4 stores to vectordb\n",
    "vectorstore  = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# step 5 create retrievalobject\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 create llm object\n",
    "llm_obj = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# step 7 prompt - Hybrid RAG\n",
    "my_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "    Use the following context to answer the question.\n",
    "    if the answer is not present in the context, answer from your general knowledge.\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# step 8: QAChain\n",
    "rag_chain = RetrievalQA.from_chain_type(llm = llm_obj,retriever=retriever_obj,chain_type_kwargs={\"prompt\":my_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4f27a9f-e113-4d84-b539-b91187ad4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is factorial?',\n",
       " 'result': 'The answer is not present in the context, so I will answer from my general knowledge.\\n\\nThe factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. In other words, it is the result of multiplying all whole numbers from n down to 1.\\n\\nFor example, the factorial of 5 (5!) is 5 Ã— 4 Ã— 3 Ã— 2 Ã— 1 = 120, which is mentioned in the given context.\\n\\nSo, in general, the factorial of a number n is calculated as:\\n\\nn! = n Ã— (n-1) Ã— (n-2) Ã— ... Ã— 2 Ã— 1'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain(\"what is factorial?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a2cb241-15ba-432d-9e7a-fc9c2e43ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In general knowledge, the factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. It is calculated as:\n",
      "\n",
      "n! = n Ã— (n-1) Ã— (n-2) Ã— ... Ã— 2 Ã— 1\n",
      "\n",
      "For example, the factorial of 5 (5!) is 120, as mentioned in the given context.\n",
      "\n",
      "So, the answer is: Factorial is the product of all positive integers less than or equal to a given number.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain(\"what is factorial?\")['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201653c-62ce-4852-a3db-fb9cc90c6d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75333a-cc2e-4ac2-8ae4-1a2c2ebe7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG with ChatHistory \n",
    "|\n",
    "# Step 1: load pdf file\n",
    "# Step 2: split -chunk \n",
    "# Step 3: Embedding\n",
    "# Step 4: VectorStores\n",
    "# Step 5: LLMobject\n",
    "# Step 6: Prompt with history\n",
    "# Step 7: QAChain\n",
    "# Step 8: Query -- Question+ChartConfigurable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9256f5f-e20c-4bc1-8c68-32ad4284cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA # Higher version of langchain -> from langchain_classic.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0685ad10-1639-4f8c-9102-8e5376ffc4dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Step-1 Load pdf file\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "## Step-2 split the docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step-3 embedding object\n",
    "embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# step 4 stores to vectordb\n",
    "vectorstore  = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# step 5 create retrievalobject\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# step 6 create llm object\n",
    "llm_obj = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "\n",
    "# Step 7 Prompt with history\n",
    "my_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "use the following context and chat history to answer the question.\n",
    "chat history:\n",
    "{history}\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\n",
    "\"\"\")\n",
    "\n",
    "# step 8: QAChain\n",
    "rag_chain = RetrievalQA.from_chain_type(llm = llm_obj,retriever=retriever_obj,chain_type_kwargs={\"prompt\":my_prompt})\n",
    "\n",
    "# Step 9 Chat history\n",
    "\n",
    "stores = {} \n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "\n",
    "rag_with_history = RunnableWithMessageHistory(rag_chain,get_session_history,input_messages_key=\"query\",history_messages_key='history')\n",
    "\n",
    "# Step 10: Create session ID and invoke rag with chathistory\n",
    "session_id = \"session-1\"\n",
    "query1 = \"What is attention in transformer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1abe26f0-15d9-46d4-a49c-c8c4cbc717c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rag_with_history.invoke({\"query\":query1},config={\"configurable\":{\"session_id\":session_id}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4ff2ec36-540f-4caf-87a1-bc93dc84d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(rag_with_history.invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bee10bf2-8832-4682-9fef-f8cea7760c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc614cb5-cba2-442a-96a2-a708d2bc1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically insert a message - at runtime?\n",
    "MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342a83-8a77-4044-b887-bb5191c0a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful assistant.\"),  <--- fixed \n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\",\"{question}\")                  |<------ dynamically list of messages       \n",
    "])                |<-- user input /current input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db1f058d-3e3e-4302-9dfc-a2bd7706e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9092a69-857b-4333-9324-a053b99f0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful assistant. Answer all the question in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"question\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6e1fc830-151e-40a8-a85a-fff1c4547f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3451aa4e-0f31-4105-83dc-04316821aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Bonjour Leo ! Je suis ravi de faire votre connaissance. Comment puis-je vous aider aujourd'hui ?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 52, 'total_tokens': 75, 'completion_time': 0.027591191, 'prompt_time': 0.007610688, 'queue_time': 0.065934522, 'total_time': 0.035201879}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5ba51db4-4ce9-4fac-b95b-614439bf6e49-0' usage_metadata={'input_tokens': 52, 'output_tokens': 23, 'total_tokens': 75}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"french\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "23e3c0bb-cd6d-486c-b3b9-1e7406d50d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Leo ! Je suis ravi de faire votre connaissance. Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f3c7984f-6f6e-47f3-9c4e-e0263f61884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤¨à¤®à¤¸à¥à¤¤à¥‡! à¤®à¥ˆà¤‚ à¤†à¤ªà¤•à¤¾ à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥‚à¤à¥¤ à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® à¤²à¤¿à¤¯à¥‹ à¤¹à¥ˆ, à¤œà¤¼à¤°à¥‚à¤° à¤¬à¤¤à¤¾à¤à¤‚, à¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤ªà¥‚à¤›à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"hindi\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5b2e1287-c8d2-48d5-9f20-6c67ee380790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à°¨à°®à°¸à±à°•à°¾à°°à°‚ à°²à±€à°“ à°…à°‚à°Ÿà±‡ à°®à°¿à°®à±à°®à°²à±à°¨à°¿ à°Žà°²à°¾ à°…à°‚à°Ÿà°¾à°°à±?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"telugu\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a813f5ad-1e15-40a4-9019-bf9b2682c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à®µà®£à®•à¯à®•à®®à¯! à®¨à®¾à®©à¯ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®‰à®¤à®µ à®‡à®™à¯à®•à¯ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯. à®¨à¯€à®™à¯à®•à®³à¯ à®²à®¿à®¯à¯‹ à®Žà®© à®‰à®™à¯à®•à®³à¯ˆ à®…à®±à®¿à®®à¯à®•à®ªà¯à®ªà®Ÿà¯à®¤à¯à®¤à®¿à®•à¯ à®•à¯Šà®£à¯à®Ÿà¯€à®°à¯à®•à®³à¯. à®¨à®²à®®à®¾?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"tamil\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c7920d2-d9ba-4df1-b0a8-324e3614c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à²¨à²®à²¸à³à²•à²¾à²° à²²à²¿à²¯à³‹! à²¨à²¿à²®à²—à³† à²¹à²¾à²—à³ à²¨à²¿à²®à³à²® à²­à²¾à²·à³†à²—à³† à²¸à²‚à²¬à²‚à²§à²¿à²¸à²¿ à²¨à²¾à²¨à³ à²¨à³€à²¡à³à²µ à²¸à²¹à²¾à²¯à²•à²°à²¾à²—à²¿à²¦à³à²¦à³‡à²¨à³†. à²¨à³€à²µà³ à²¯à²¾à²µ à²¬à²—à³à²—à³† à²ªà³à²°à²¶à³à²¨à³†à²—à²³à²¨à³à²¨ à²•à³‡à²³à²²à³ à²¬à²¯à²¸à³à²¤à³à²¤à³€à²°à²¾?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"kannada\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780b69-e722-4187-b104-5768e9b88bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65140d7e-e538-4dd5-8a4f-04a885cbc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python <---------------------------> Database\n",
    "|->DS+class+object+method                  |->SQL\n",
    "      --------------------------------->\n",
    "      <----------------DBI-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "04399b4f-b433-4d32-b892-9f9a589ab01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Connection at 0x1d4aea6d6c0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "sqlite3.connect(\"test1.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "afdad2c0-4056-48d9-8551-bee21215005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4a657e540>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"test1.db\")\n",
    "conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d6d31ae-0605-482d-b3bf-74738301e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1c13cad-2492-444e-b2b3-e585af70f1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4ac88eb40>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"create table product(id NUMBER,pname TEXT,pcost NUMBER)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "16d9ebf0-5c63-455d-91c4-e761f897c602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4ac88eb40>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"insert into product(id,pname,pcost) values(101,'pA',1000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a416f7fa-4e18-4870-a1a3-2685dab46238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4ac88eb40>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"insert into product(id,pname,pcost) values(102,'pB',2000)\")\n",
    "sth.execute(\"insert into product(id,pname,pcost) values(103,'pC',3000)\")\n",
    "sth.execute(\"insert into product(id,pname,pcost) values(103,'pD',4000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d87304b0-e40f-4306-bcfc-7c4bc028d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4ac88eb40>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = 105\n",
    "vb = 'pE'\n",
    "vc = 4500\n",
    "sth.execute(\"insert into product(id,pname,pcost) values(?,?,?)\",(va,vb,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a950f1c-dfbc-4755-8458-fac457c1658e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d4ac88eb40>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"select *from product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98222c87-838d-4bd7-a123-f01bc4bbba14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 'pA', 1000)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "08c1a640-82c2-4494-9374-d0f6a1a2b2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 'pB', 2000)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b1677747-9c9d-4c0e-b48a-c50a3ecc9607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 'pC', 3000)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c82cd1d4-117d-4e1f-8551-9203508d4db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 'pD', 4000)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "250198fb-a169-4fb0-8c73-671385177ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 'pE', 4500)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "79c9bed0-ea90-4114-aff8-c5c8ec49a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sth.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7950eccc-8a8a-47ba-b4d3-0c3b699a7f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 'pA', 1000),\n",
       " (102, 'pB', 2000),\n",
       " (103, 'pC', 3000),\n",
       " (103, 'pD', 4000),\n",
       " (105, 'pE', 4500)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"select *from product\")\n",
    "sth.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3e4dc09d-c495-47af-985e-7fff55c0669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 'pA', 1000),\n",
       " (102, 'pB', 2000),\n",
       " (103, 'pC', 3000),\n",
       " (103, 'pD', 4000),\n",
       " (105, 'pE', 4500)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.execute(\"select *from product\")\n",
    "list(sth) # generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5d549bcc-97eb-4f8b-bdd4-02cfe125a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37205965-ae21-4f51-9c13-3c9e3686aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f9481-abb2-4cbd-a097-82a83b565451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################  End of the Day3 ################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
