{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b13225-0089-4feb-bb25-ecd2dbc7b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b76d87d-d62e-4077-bd79-da7defd5771d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e30ff0-09ed-45f2-baa1-1e6da510b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response-1: content=\"In the context of the Transformer model architecture, attention refers to the multi-head self-attention mechanism used in both the encoder and decoder stacks. This mechanism allows the model to weigh the importance of different input elements and focus on the most relevant information when generating output.\\n\\nIn the Transformer, attention is used in three different ways:\\n\\n1. Self-attention: The model computes the attention weights among all elements in the input sequence to weigh their importance.\\n2. Encoder-decoder attention: The decoder attends to the output of the encoder to generate the final output.\\n3. Encoder-encoder attention: The model attends to its own output at different positions.\\n\\nThe Transformer uses multi-head attention, which involves splitting the input into multiple attention heads and processing them in parallel. Each head computes a weighted sum of the input elements, and the final output is a weighted sum of the outputs from all heads.\\n\\nThe key components of attention in the Transformer are:\\n\\n* Query (Q), Key (K), and Value (V) vectors: These are the input vectors that are used to compute the attention weights.\\n* Attention weights: These are the weights that are computed based on the dot product of the query and key vectors.\\n* Output vector: This is the final output vector that is computed by taking a weighted sum of the value vectors based on the attention weights.\\n\\nThe Transformer's use of attention allows it to model complex dependencies between input elements and to generate output that is conditioned on the input.\" additional_kwargs={} response_metadata={}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Step-1 Load pdf file\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "## Step-2 split the docs \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "## Step-3 Embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "## Step-4 Vectorstores\n",
    "vector_store = FAISS.from_documents(docs,embeddings)\n",
    "retriever_obj = vector_store.as_retriever()\n",
    "\n",
    "## Step-5 LLM object\n",
    "llm_obj = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "##  Step 6: Prompt with history\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context and chat history to answer the question.\n",
    "\n",
    "Chat history:\n",
    "{history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "\n",
    "## Step 7: Build stuff + retrieval chain\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_obj,qa_chain)\n",
    "\n",
    "def convert_output_to_aimessage(output):\n",
    "    return AIMessage(content=output[\"answer\"])\n",
    "\n",
    "final_rag_chain = rag_chain | convert_output_to_aimessage\n",
    "\n",
    "## Step 8: Add RunnableWithMessageHistory \n",
    "stores = {}\n",
    "def get_session_history(session_id: str) ->BaseChatMessageHistory:\n",
    "    if session_id not in stores:\n",
    "        stores[session_id] = ChatMessageHistory()\n",
    "    return stores[session_id]\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    final_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "## Step 9: Create session ID and invoke rag with chatHistory\n",
    "\n",
    "session_id = \"session-1\"\n",
    "query1 = \"What is attention in transformer?\"\n",
    "\n",
    "response1 = rag_with_history.invoke({\"input\":query1},config={\"configurable\":{\"session_id\":session_id}})\n",
    "\n",
    "print(\"response-1:\",response1)\n",
    "print('')\n",
    "query2 = \"Explain again in shortly\"\n",
    "response2 = rag_with_history.invoke({\"input\":query2},config={\"configurable\":{\"session_id\":session_id}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3af391-c4fa-4bce-a78a-7c3ec2f32097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='In the context of the Transformer model architecture, attention refers to the multi-head self-attention mechanism. It weighs the importance of different input elements and focuses on the most relevant information when generating output. \\n\\nThis mechanism is used in three ways:\\n\\n1. Self-attention: The model computes attention weights among all elements in the input sequence.\\n2. Encoder-decoder attention: The decoder attends to the output of the encoder.\\n3. Encoder-encoder attention: The model attends to its own output at different positions.\\n\\nSelf-attention is a key component of attention in the Transformer, which allows the model to model complex dependencies between input elements.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53751e50-b2e4-4682-836a-edba9f9e3de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'session-1': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is attention in transformer?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In the context of the Transformer model architecture, attention refers to the multi-head self-attention mechanism used in both the encoder and decoder stacks. This mechanism allows the model to weigh the importance of different input elements and focus on the most relevant information when generating output.\\n\\nIn the Transformer, attention is used in three different ways:\\n\\n1. Self-attention: The model computes the attention weights among all elements in the input sequence to weigh their importance.\\n2. Encoder-decoder attention: The decoder attends to the output of the encoder to generate the final output.\\n3. Encoder-encoder attention: The model attends to its own output at different positions.\\n\\nThe Transformer uses multi-head attention, which involves splitting the input into multiple attention heads and processing them in parallel. Each head computes a weighted sum of the input elements, and the final output is a weighted sum of the outputs from all heads.\\n\\nThe key components of attention in the Transformer are:\\n\\n* Query (Q), Key (K), and Value (V) vectors: These are the input vectors that are used to compute the attention weights.\\n* Attention weights: These are the weights that are computed based on the dot product of the query and key vectors.\\n* Output vector: This is the final output vector that is computed by taking a weighted sum of the value vectors based on the attention weights.\\n\\nThe Transformer's use of attention allows it to model complex dependencies between input elements and to generate output that is conditioned on the input.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain again in shortly', additional_kwargs={}, response_metadata={}), AIMessage(content='In the context of the Transformer model architecture, attention refers to the multi-head self-attention mechanism. It weighs the importance of different input elements and focuses on the most relevant information when generating output. \\n\\nThis mechanism is used in three ways:\\n\\n1. Self-attention: The model computes attention weights among all elements in the input sequence.\\n2. Encoder-decoder attention: The decoder attends to the output of the encoder.\\n3. Encoder-encoder attention: The model attends to its own output at different positions.\\n\\nSelf-attention is a key component of attention in the Transformer, which allows the model to model complex dependencies between input elements.', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c570c-9ed7-4aca-a478-e2ddede1b765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f3186-3dbb-4f3a-833d-11a47f7d6b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6fef9-4333-4fba-9726-6b7e6e14052d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
